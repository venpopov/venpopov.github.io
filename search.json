[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Popov, V. & Oberauer, K. (submitted). How to build an observatory for the mind\nTrueblood, J. S., Allison, D., Field, S. M., Fishbach, A., Gaillard, S. D. M., Gigerenzer, G., ‚Ä¶ Popov, V.,‚Ä¶ Teodorescu, A. (submitted). The Misalignment of Incentives in Academic Publishing and Implications for Journal Reform. Preprint available at OSF\nBinz, M., Alaniz, S., Roskies, A., Aczel, B., Bergstrom, C. T., Allen, C., ‚Ä¶ Popov, V. ‚Ä¶ Schulz, E. (submitted). How should the advent of large language models affect the practice of science?. Preprint available at OSF\nPopov, V. (under review). Cognitive resources can be intentionally released when processed information becomes irrelevant: Insights from the primacy effect in working memory. Preprint available at PsyArXiv\nPopov, V. (under revision). If God Handed Us the Ground-Truth Theory of Memory, How Would We Recognize It? Preprint available at OSF\nMa, S., Popov, V. , & Zhang, Q. (under revision). A Neural Index Reflecting the Amount of Cognitive Resources Available during Memory Encoding: A Model-based Approach. Preprint available at biorxiv\nFrischkorn, G.* & Popov, V.* (co-first authors) (under revision). A tutorial for estimating mix- ture models for visual working memory tasks in brms: Introducing the Bayesian Measurement Modeling (bmm) package for R. Preprint available at psyarxiv"
  },
  {
    "objectID": "publications.html#works-in-progress",
    "href": "publications.html#works-in-progress",
    "title": "Publications",
    "section": "",
    "text": "Popov, V. & Oberauer, K. (submitted). How to build an observatory for the mind\nTrueblood, J. S., Allison, D., Field, S. M., Fishbach, A., Gaillard, S. D. M., Gigerenzer, G., ‚Ä¶ Popov, V.,‚Ä¶ Teodorescu, A. (submitted). The Misalignment of Incentives in Academic Publishing and Implications for Journal Reform. Preprint available at OSF\nBinz, M., Alaniz, S., Roskies, A., Aczel, B., Bergstrom, C. T., Allen, C., ‚Ä¶ Popov, V. ‚Ä¶ Schulz, E. (submitted). How should the advent of large language models affect the practice of science?. Preprint available at OSF\nPopov, V. (under review). Cognitive resources can be intentionally released when processed information becomes irrelevant: Insights from the primacy effect in working memory. Preprint available at PsyArXiv\nPopov, V. (under revision). If God Handed Us the Ground-Truth Theory of Memory, How Would We Recognize It? Preprint available at OSF\nMa, S., Popov, V. , & Zhang, Q. (under revision). A Neural Index Reflecting the Amount of Cognitive Resources Available during Memory Encoding: A Model-based Approach. Preprint available at biorxiv\nFrischkorn, G.* & Popov, V.* (co-first authors) (under revision). A tutorial for estimating mix- ture models for visual working memory tasks in brms: Introducing the Bayesian Measurement Modeling (bmm) package for R. Preprint available at psyarxiv"
  },
  {
    "objectID": "publications.html#journal-publications",
    "href": "publications.html#journal-publications",
    "title": "Publications",
    "section": "Journal publications",
    "text": "Journal publications\n\n\n\n2024\n\n\nDames, H., Musfeld, P., Popov, V., Oberauer, K., & Frischkorn, G. (in press). Responsible Research Assessment Should Prioritize Theory Development and Testing Over Ticking Open Science Boxes. Meta-psychology. Preprint available at https://psyarxiv.com/ad74m/\n\n\n\n\n2023\n\n\nDames, H. & Popov, V. (2023). When does intenet matter for memory? Bridging perspectives with Craik. Journal of Experimental Psychology: General. 152(11), 3300‚Äì3309\n\n\n\n\n\n\n\n\nPopov, V. & Dames, H. (2023). Intent Matters: Resolving the Intentional vs Incidental Learning Paradox in Episodic Long-term Memory. Journal of Experimental Psychology: General. [Preprint] [Data & Code]\n\n\n\n\n2022\n\n\nNorton, C. M., Ibinson, J. W., Pcola, S. J., Popov, V., Tremel, J. J., Reder, L. M., ‚Ä¶ & Vogt, K. M. (2022). Neutral auditory words immediately followed by painful electric shock may show reduced next-day recollection. Experimental Brain Research, 1-13.\n\n\n\n\n2021\n\n\nPopov, V., So, M. & Reder, L. (2021). Memory resources recover gradually over time: The effects of word-frequency, presentation rate and list-composition on binding errors and mnemonic precision in source memory. Journal of Experimental Psychology: Learning, Memory & Cognition. [PDF][Data & Code]\n\n\n\n\n\n\n\n\nVogt, K., Ibinson, J., Smith, C., Citro, A., Norton, C., Karim, H., Popov, V., Mahajan, A., Aizenstein, H., Reder, L. & Fiez, J. (2021). Midazolam and ketamine produce distinct neural changes in memory, pain, and fear networks during pain. Anesthesiology 135 (1), 69-82\n\n\n\n\n2020\n\n\nPopov, V. & Reder, L. (2020). Greater discrimination difficulty during perceptual learning leads to stronger and more distinct representations. Psychonomic Bulletin & Review. Advance Online Publication. [PDF] [Data & Code]\n\n\n\n\n\n\n\n\nPopov, V.¬†& Reder, L. (2020). Frequency Effects on Memory: A Resource-Limited Theory. Psychological Review.¬†127(1),¬†1‚Äì46.¬†¬†[PDF]¬†[Preprint]¬†[Data & Code]\n\n\n\n\n\n\n\n\nVassileva, J., Psedarska, E., Yankov, G., Bozgunov, K.,¬†Popov, V.¬†& Vasilev, G. (2020). Validation of the Levenson Self-Report Psychopathy Scale in Bulgarian Substance Dependent Individuals.¬†Frontiers in Psychology\n\n\n\n\n2019\n\n\nPopov, V., Marevic, I., Rummel, J. & Reder, L. (2019). Forgetting is a Feature, not a Bug: Intentionally Forgetting Some Things Helps Us Remember Others by Freeing up Working Memory Resources. Psychological Science.¬†30(9), 1303-1317.¬†[PDF]¬†[Preregistration]¬†[[Data, Stimuli & Code]]\n\n\n\n\n\n\n\n\nPopov, V., Zhang, Q., Koch, G., Halloway, R. & Coutanche, M. (2019). Semantic knowledge influences whether novel episodic associations are represented symmetrically or asymmetrically. Memory & Cognition. Advanced Online Publication¬†[Preregistration]¬†[Preprint]¬†[Stimuli, data & code]\n\n\n\n\n2018\n\n\nPopov, V.*, Ostarek, M.*, & Tenison, C. (2018). Practices and Pitfalls in Inferring Neural Representations. NeuroImage, 174, 340-351. [PDF] [Preprint] [Code]\n\n\n\n\n\n\n\n\nShen, Z.*, Popov, V.* (co-first authors), Delahay, A., & Reder, L. (2018). Item Strength Affects Working Memory Capacity. Memory & Cognition, 46(2), 204-215.¬†[PDF]\n\n\n\n\n2017\n\n\nPopov, V., Hristova, P., & Anders, R. (2017). The Relational Luring Effect: Retrieval of relational information during associative recognition.¬†Journal of Experimental Psychology: General, 146(5), 722-745¬†[PDF]\n\n\n\n\n\n\n\n\nManelis, A.*,¬†Popov, V.* (co-first authors), Paynter, C., Walsh, M., Wheeler, M., Vogt, K., & Reder, L. (2017). Cortical Networks Involved in Memory for Temporal Order.¬†Journal of Cognitive Neuroscience, 29(7), 1253-1266.¬†[PDF]\n\n\n\n\n2016\n\n\nReder, L. M., Liu, X. L., Keinath, A., &¬†Popov, V.¬†(2016). Building knowledge requires bricks, not sand: The critical role of familiar constituents in learning.¬†Psychonomic Bulletin & Review, 23(1), 271-277.¬†[PDF][Data]\n\n\n\n\n2015\n\n\nPopov, V. & Hristova, P. (2015). Unintentional and efficient relational priming. Memory & Cognition, 46(6), 866-878. [PDF]"
  },
  {
    "objectID": "publications.html#refereed-full-conference-papers",
    "href": "publications.html#refereed-full-conference-papers",
    "title": "Publications",
    "section": "Refereed full conference papers",
    "text": "Refereed full conference papers\n\n\n\n2018\n\n\nZhang, Q.*,¬†Popov, V.*¬†(co-first authors), Koch, G., Halloway, R. & Coutanche, M. (2018). Fast Memory Integration Facillitated by Schema Consistency. In C. Kalish, M. Rau, J. Zhu, T. Rogers (Eds.),¬†Proceedings of the 40th Annual Conference of the Cognitive Science Society.¬†Austin, TX: Cognitive Science Society.\n\n\n\n\n2017\n\n\nPopov, V. , Ostarek, M., & Tenison, C. (2017). Inferential Pitfalls in Decoding Neural representations. In G. Gunzelmann, A. Howes, T. Tenbrink, & E. Davelaar (Eds.),¬†Proceedings of the 39th Annual Conference of the Cognitive Science Society¬†(pp.¬†961-966). Austin, TX: Cognitive Science Society.¬†\n\n\n\n\n\n\n\n\nPopov, V. , & Reder, L. (2017). Target-to-distractor similarity can help visual search performance. In G. Gunzelmann, A. Howes, T. Tenbrink, & E. Davelaar (Eds.),¬†Proceedings of the 39th Annual Conference of the Cognitive Science Society¬†(pp.¬†968‚Äì973). Austin, TX: Cognitive Science Society.¬†PDF\n\n\n\n\n\n\n\n\nPopov, V. , & Reder, L. (2017). Repetition improves memory by strengthening existing traces: Evidence from paired-associate learning under midazolam. In G. Gunzelmann, A. Howes, T. Tenbrink, & E. Davelaar (Eds.),¬†Proceedings of the 39th Annual Conference of the Cognitive Science Society¬†PDF¬†(pp.¬†2913-2918). Austin, TX: Cognitive Science Society.\n\n\n\n\n2014\n\n\nPopov, V.¬†& Hristova, P. (2014). Automatic analogical reasoning underlies structural priming in comprehension of ambiguous sentences. In P. Bello, M. Guarini, M. McShane, & B. Scassellati (Eds.),¬†Proceedings of the 36th Annual Conference of the Cognitive Science Society¬†(pp.¬†1192-1197). Austin, TX: Cognitive Science Society.¬†PDF\n\n\n\n\n\n\n\n\nPopov, V.¬†& Petkov, G. (2014). The level of processing affects the magnitude of induced retrograde amnesia. In P. Bello, M. Guarini, M. McShane, & B. Scassellati (Eds.),¬†Proceedings of the 36th Annual Conference of the Cognitive Science Society¬†(pp.¬†2787-2792). Austin, TX: Cognitive Science Society."
  },
  {
    "objectID": "publications.html#publications-in-bulgarian",
    "href": "publications.html#publications-in-bulgarian",
    "title": "Publications",
    "section": "Publications in Bulgarian",
    "text": "Publications in Bulgarian\n\n\n\n2016\n\n\nPopov, V., Nedelchev, D., Peneva, E., Psederska, E., Georgieva, V., Vasilev, G., Vassileva, J. (2016). Psychometric Characteristics of the Bulgarian Version of the Buss-Warren Aggression Questionnaire (BWAQ).¬†Clinical and Consulting Psychology, 4(30), 37-53.\n\n\n\n\n\n\n\n\nPopov, V., Psederska, E., Peneva, E., Bozgunov, K., Vasilev, G., Nedelchev, D., & Vassileva, J. (2016). Psychometric Characteristics of the Bulgarian Version of the Toronto Alexithymia Scale (TAS-20). Psychological Research, 19(2), 25-42 (in Bulgarian)\n\n\n\n\n\n\n\n\nNedelchev, D.,¬†Popov, V., Psedarska, E., Bozgunov, K., Vasilev, G., Peneva, E., & Vassileva, J. (2016). Psychometric Characteristics of the Bulgarian Version of the Wender Utah Rating Scale (WURS-25) for ADHD.¬†Clinical and Consulting Psychology, 8(2), 3-17 (in Bulgarian)\n\n\n\n\n2015\n\n\nPopov, V., Bozgunov, K., Vasilev, G., & Vassileva, J. (2015). Psychometric Characteristics of the Bulgarian Version of Levenson‚Äôs Self-report Psychopathy Scale. Bulgarian Journal of Psychology, 1-4, 253-278 (in Bulgarian)"
  },
  {
    "objectID": "publications.html#book-chapters",
    "href": "publications.html#book-chapters",
    "title": "Publications",
    "section": "Book chapters",
    "text": "Book chapters\nPopov, V. & Reder, M. (in press). Frequency effects in recognition and recall. To appear In M. Kahana & A. Wagner (Eds.),¬†Handbook on Human Memory. Oxford University Press¬†[PDF]"
  },
  {
    "objectID": "publications.html#unpublished-manuscripts",
    "href": "publications.html#unpublished-manuscripts",
    "title": "Publications",
    "section": "Unpublished manuscripts",
    "text": "Unpublished manuscripts\nPopov, V.,¬†Pavlova, M. & Hristova, P. (2020). The Internal Structure of Semantic Relations: Effects of Relational Similarity and Typicality.¬†PsyArxiv. [Preprint]"
  },
  {
    "objectID": "posts/2026/package-management-and-reproducibility-are-a-nightmare/index.html",
    "href": "posts/2026/package-management-and-reproducibility-are-a-nightmare/index.html",
    "title": "Package management and reproducibility are a nightmare",
    "section": "",
    "text": "I try my best. I really do.\nI use renv for package management. My blog is built with Quarto. I use caching and freezing so posts don‚Äôt get recomputed every time. I hadn‚Äôt touched the blog in a few months. Today, I just wanted to fix a couple of typos on my publications page. Two minutes of work. Run quarto render. Done.\nExcept, of course, not done.\nI open the project and renv tells me the project is out of sync. Cannot render because package Rmarkdown is missing. Fine. I run renv::restore(), whose entire purpose is to restore the package environment to its previous state.\nIt immediately fails while trying to install Matrix from source.\nI‚Äôve never had that problem before. A bit of googling reveals that installing Matrix requires a Fortran compiler (???). The official instructions are here: https://mac.r-project.org/tools/. I do not remember ever needing this in the past. I try to force renv to use binaries instead of building from source. That also fails.\nAlright. Fine. I install a Fortran compiler. Matrix installs successfully.\nOne minute later, renv::restore() fails again‚Äîthis time because it can‚Äôt install stringi. The error message is completely opaque. I paste it into an AI, which helpfully explains:\n\n‚ÄúThis error occurs because of a conflict between the modern [[noreturn]] attribute in R 4.5 and an older NORET declaration within the stringi package, specifically on ARM64 macOS.‚Äù\n\nJesus fucking christ.\nYes, I updated R last month from 4.4.2 to 4.5. And now one of the most fundamental string manipulation packages in the entire R ecosystem‚Äîon which half the world depends‚Äîcan‚Äôt be installed?\nOkay. Fine. Surely I can just install a newer version of stringi. I stop the restore process and run:\nrenv::install(\"stringi\")\nThat works.\nBut now I have a problem: if I run renv::restore() again, it will just try to install the older version and fail all over again. So I need to snapshot the current state.\nI run renv::snapshot(), and it tells me:\nThe following required packages are not installed:\n- ambient\n- bmm\n- fields\n- ggdark\n- ggplot2\n- here\n- httpgd\n- kableExtra\n- patchwork\n- rmarkdown\n- tidyr\n- viridis\n- wesanderson\nPackages must first be installed before renv can snapshot them.\nWhy are they not installed? I‚Äôm on the same machine. Same project. I haven‚Äôt changed anything.\nOh. Right. I updated R. Since R is a global installation, renv considers this a fresh library and wants everything reinstalled.\nFine. I choose option 2: install the packages, then snapshot.\nEverything installs‚Äîexcept ggdark.\nError: package 'ggdark' is not available\nWhat.\nA quick google search leads me to CRAN, which cheerfully explains:\nPackage ‚Äòggdark‚Äô was removed from the CRAN repository.\nFormerly available versions can be obtained from the archive.\nArchived on 2025-11-07 as issues were not corrected despite reminders.\nOh for fuck‚Äôs sake.\nI recognize this exact message. It recently bit me with another ggplot utility package, gghalves. ggplot just went through a major version bump to v4, and it broke a bunch of extension packages. If the maintainer doesn‚Äôt fix things quickly enough, CRAN just removes the package.\nUnfortunately, R‚Äôs entire package ecosystem is built on the assumption that every package should work with the latest version of every other package. That assumption collapses completely the moment you care about reproducibility.\nThe whole point of renv is that I‚Äôve already specified which versions of ggplot2 and ggdark this project used. When I chose ‚Äúinstall the packages‚Äù during snapshotting, I assumed renv would install the versions recorded in the lockfile.\nBut something clearly didn‚Äôt work.\nWhat exactly failed?\n\nDid renv try to download the correct version of ggdark, but CRAN refused because it‚Äôs archived?\nOr did renv ignore the lockfile at that point and try to install the latest version instead?\n\nCRAN does link to the archived sources, so in principle the old version exists. But apparently not in a way that renv can use automatically.\nI snapshot without installing the missing packages. That technically works, but I‚Äôm stuck with an inconsistent environment.\nI‚Äôm not even using ggplot v4. This isn‚Äôt a usage problem. It‚Äôs an infrastructure problem: R‚Äôs dependency system forces compatibility with the present, and fights you when you try to live in the past.\nFine. I try installing from the archive manually:\nrenv::install(\"ggdark@0.2.1\")\nThat works.\nI try renv::snapshot() again.\nNow it fails on httpgd.\nSame story. Package removed from CRAN. Archived. The version in my lockfile is httpgd@2.0.2.\nrenv::install(\"httpgd@2.0.2\")\nWorks.\nWhy am I doing this manually? Isn‚Äôt this exactly what renv is supposed to manage for me?\nAt this point I start wondering whether this has something to do with Quarto. Some posts use post-specific packages declared via renv::use(\"packagename@version\"), as recommended in the documentation. Maybe those dependencies don‚Äôt fully participate in the main restore/snapshot logic? Maybe I‚Äôve built myself a beautiful little Rube Goldberg machine of reproducibility.\nOne last attempt:\nrenv::snapshot()\nFinally:\nrenv::status()\nNo issues found -- the project is in a consistent state.\nWhat time is it?\nTwo hours have passed. I wanted to fix two typos.\nI run quarto render.\nHallelujah‚Äîit works.\nI need a drink.\nNever mind. I forgot I don‚Äôt drink.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{popov2026,\n  author = {Popov, Vencislav},\n  title = {Package Management and Reproducibility Are a Nightmare},\n  date = {2026-02-03},\n  url = {https://venpopov.com/posts/2026/package-management-and-reproducibility-are-a-nightmare/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPopov, Vencislav. 2026. ‚ÄúPackage Management and Reproducibility\nAre a Nightmare.‚Äù February 3, 2026. https://venpopov.com/posts/2026/package-management-and-reproducibility-are-a-nightmare/."
  },
  {
    "objectID": "posts/2025/open-science-needs-reliable-infrastructure/index.html",
    "href": "posts/2025/open-science-needs-reliable-infrastructure/index.html",
    "title": "Open Science needs reliable infrastructure",
    "section": "",
    "text": "In October 2025, a redesign of the Open Science Framework (OSF) led to widespread access failures across the platform. What began as a few broken download links became, in my case, a total disappearance of eight years of DOI-registered work. This is the story of what happened, how it was resolved, and what it reveals about trust and infrastructure in open science."
  },
  {
    "objectID": "posts/2025/open-science-needs-reliable-infrastructure/index.html#widespread-glitches",
    "href": "posts/2025/open-science-needs-reliable-infrastructure/index.html#widespread-glitches",
    "title": "Open Science needs reliable infrastructure",
    "section": "Widespread glitches",
    "text": "Widespread glitches\nOn October 11th, 2025, the Center for Open Science (COS) announced that the Open Science Framework ‚Äî its flagship platform for preprints, data, and code ‚Äî had received a major redesign. The update promised ‚Äúto make managing research projects easier, faster, and more intuitive.‚Äù\nWithin days, many discovered that the new redesign ironically came with slower loading times, but more troubling, with a host of bugs and glitches:\n\n\nPSA: seems like the update broke the ability to directly download files with a link, like osf.io/XXXX/download this lead to an error in one of my R package vignettes, and presumably will break lots of code.[image or embed]\n\n‚Äî Ruben C. Arslan (@ruben.the100.ci) October 12, 2025 at 10:34 AM\n\n\n\nThanks. I think this is urgent. I suspect it also broke Google Scholar listing PDFs from OSF, at least I get broken links for some, no links for others and generally OSF seems to be massively downranked in the list of sources. Started getting email requests for PDFs again.\n\n‚Äî Ruben C. Arslan (@ruben.the100.ci) October 16, 2025 at 11:43 AM\n\n\n\n@cos.io Why cant I read in rds files from my repo? I was trying to reproduce my manuscript and now I cant read in rds files from the site. I am going to assume it has to do with the new UI?\n\n‚Äî jgeller1phd.bsky.social (@jgeller1phd.bsky.social) October 11, 2025 at 5:57 PM\n\n\n\nIs anyone else experiencing significant issues (lags, not loading) with OSF (@cos.io) since the interface update?\n\n‚Äî Charlotte Pennington üåª (@drcpennington.bsky.social) October 22, 2025 at 7:51 PM\n\n\n\nü™≤ OSF Update: Ongoing Fixes Following the OSF redesign, some issues have been identified:\n\nSome custom registry templates may not showall fields\nOSF shows a max. of 10 contributors\nSome pages load slowly or seem unresponsive\n\nThese are on our critical fix list & should be resolved soon. (1/2)\n\n‚Äî Center for Open Science (@cos.io) October 17, 2025 at 7:46 PM\n\n\n\nAlso, - Login screen doesn‚Äôt work on Safari - Preregistrations impossible (got many error messages & after I was finally able to submit got an email saying trouble archiving my preregistration so it wasn‚Äôt completed)‚Ä¶ this means I can‚Äôt actually run my study :( - Symbols not rendering (e.g., &, &lt;)\n\n‚Äî Cassandra Chapman (@cassandrachapman.bsky.social) October 22, 2025 at 11:38 PM\n\n\nI do not mention complaints about design, as those are subjective. But it was worrisome that a redesign would break existing download link patterns without a redirect - this should not happen to an archival service. To their credit, COS fixed that particular problem quickly, though it should never needed fixing in the first place. The performance issues remain, and the irony that this performance update made performance worse is not lost to anyone. What seemed like transitional friction soon turned into something stranger."
  },
  {
    "objectID": "posts/2025/open-science-needs-reliable-infrastructure/index.html#the-disappearance",
    "href": "posts/2025/open-science-needs-reliable-infrastructure/index.html#the-disappearance",
    "title": "Open Science needs reliable infrastructure",
    "section": "The Disappearance",
    "text": "The Disappearance\nOn November 2, I realized that none of my OSF preprints would open. Every single one ‚Äî roughly twenty projects spanning eight years ‚Äî returned either a blank page with a red banner reading ‚ÄúNot found‚Äù or a JSON message declaring the resource ‚Äúdeleted by the user.‚Äù\nAt first, I assumed it was a global outage consistent with the widespread performance issues mentioned above. But when I double checked, the pattern was unmistakable:\n\nOther authors‚Äô preprints worked fine.\nThe error appeared for every preprint and dataset linked to my name ‚Äî including one uploaded by a colleague just four days earlier.\nEven Google Scholar links that had functioned for years now led to ‚ÄúResource deleted‚Äù errors:\n\n{\"message_short\": \"Resource deleted\", \"message_long\": \"User has deleted this content. If this should not have occurred and the issue persists, please report it to &lt;a href=\\\"mailto:support@osf.io\\\"&gt;support@osf.io&lt;\\/a&gt;.\", \"code\": 410, \"referrer\": null}\nMost worrisome of all - the ‚Äúblackout‚Äù included files linked in published journal articles and materials for projects currently under ¬†peer review.\nI reported the issue to OSF support with screenshots and links, expecting acknowledgment that something had gone wrong with the migration:\n\n&lt;On Nov 2, 2025 at 23:40 +0100, Ven Popov, wrote:&gt;\nI am experiencing a serious issue with my preprints on OSF. At first I thought this was a global OSF issue, but it seems like it is affecting all and only my preprints, which makes me very concerned.\nWhen I try to access any of the preprints on which I am a co-author, I get an error.\nIf I try to access the preprint via and OSF url, I get a blank page with a red banner ‚ÄúNot found‚Äù (see attached screenshot). Examples:\n\nhttps://osf.io/yr9xb\nhttps://osf.io/gwd4s\nhttps://osf.io/hmu9r\nhttps://osf.io/dsx6y\nhttps://osf.io/vj8pn (this one was uploaded only 4 days ago by a colleague!)\n\nIf I try to access the PDF links on Google Scholar (e.g.¬†this link), I get the following JSON error message:\n[‚Ä¶]\nI tested this with all ~20 preprints listed on my profile and I get the error on all of them. When I try to open any other preprint that is not authored by me, I can open it just fine. I‚Äôd appreciate any assistance with resolving this issue.\n\nInstead, I was asked to fill out a form for ‚Äúaccounts flagged as spam.‚Äù\n\n&lt; On Mon, Nov 3, 2025 at 7:30 AM EST, Blaine Support support@osf.io wrote:&gt;\nIf you received this email, it means that we need your assistance in investigating your account. Please help us by providing more information through this form: Account Disabled or Falsely Flagged as SPAM reporting. If we don‚Äôt receive your form submission, we won‚Äôt be able to investigate the issue. Your input is invaluable as we work to improve our spam detection systems."
  },
  {
    "objectID": "posts/2025/open-science-needs-reliable-infrastructure/index.html#the-support-loop",
    "href": "posts/2025/open-science-needs-reliable-infrastructure/index.html#the-support-loop",
    "title": "Open Science needs reliable infrastructure",
    "section": "The Support Loop",
    "text": "The Support Loop\nI filled out the spam-flag form, though I knew my account was active and clean. I was logged into my account and could change my account information. I could also see all my project listings, though not the details. I replied back:\n\n&lt; On Mon, Nov 3, 2025 at 7:47 AM EST, Ven Popov, wrote:&gt;\nI filled out the form, but my account is neither deactivated nor flagged as SPAM. I simply cannot access any of the 20+ preprints that I or my colleagues have uploaded over the last 8 years! In the form I supplied a link to one preprint, but all of my preprints are inaccessible.\n\nA few hours later I received a message from a new support representative, asking me again to fill out the same form:\n\n&lt; On Nov 3, 2025 at 16:04 +0100, Michaela Support support@osf.io, wrote:&gt;\nPlease fill out the form: Account Disabled or Falsely Flagged as SPAM reporting. Your preprint have been flagged as spam. This is an important step in tracking how effective (or not) our spam filters are, and to review and resolve the issue.\n\nFrustrated, I copied Brian Nosek on a detailed escalation email explaining that this was not a single-ticket problem but a catastrophic integrity failure: twenty preprints, hundreds of files, dozens of co-authors, and years of published work were all inaccessible. It was not one ‚Äúpreprint flagged as spam.‚Äù Anything I ‚Äî or anyone associated with me ‚Äî had uploaded since 2017 was completely inaccessible, and OSF was effectively communicating to the world that this work no longer existed.\n\n&lt;On Nov 3, 2025 at 16:45 +0100, Ven Popov, wrote:&gt;\nAs I replied in my previous email, I already filled out the form.\nFurthermore, it is not one preprint that is inaccessible. All of the 20+ preprints that I have published on OSF since 2017 are unavailable. Neither the OSF links from my profile, nor the Google Scholar links that have worked for years can be opened.\nPlease escalate this issue. This is a serious misstep on OSF‚Äôs part. I have had an account with OSF for 8 years, I have published about 20 preprints during this time, and none of them are available. For some of them, the OSF links might be the only record online of these published works, and it is a serious issue concerning the longevity of scholarship that an archival platform like OSF cannot allow to happen.\n@Brian, can you please get your support team to take this seriously? I‚Äôm getting repeated emails asking me to do something I already did and overlooking the extent of the problem. Please see the message history for the detailed report and links to my profile, a subset of broken preprint links, and an API error from Google Scholar links.\n\nWithin hours, OSF staff identified the cause: my account had been automatically flagged as spam by their filters. Once the flag was removed, everything reappeared."
  },
  {
    "objectID": "posts/2025/open-science-needs-reliable-infrastructure/index.html#the-explanation",
    "href": "posts/2025/open-science-needs-reliable-infrastructure/index.html#the-explanation",
    "title": "Open Science needs reliable infrastructure",
    "section": "The Explanation",
    "text": "The Explanation\nHere‚Äôs the message I received:\n\n&lt; On Nov 3, 2025 at 19:31 +0100, Michaela Support support@osf.io, wrote&gt;\nThank you for reaching out to the OSF support team! You are correct it looks like your project was caught in our spam filter. As a free open access platform we are frequently the target of spammers, and therefore we must use multiple different spam filters that take a plethora of different factors into account when flagging spam, everything from odd titles to suspicious activity from a nearby IP address could get flagged. Truthfully it‚Äôs hard to tell why your content got its attention. I‚Äôm sorry for the inconvenience. I‚Äôve removed the flag from material and your content should be active again!\n\nIt was an honest reply ‚Äî but also a disturbing one.\nOSF‚Äôs filters had quarantined everything I or any of my colleagues had ever uploaded, without warning, without a visible flag on my dashboard, and without preserving any metadata or landing pages. Public DOIs and Google Scholar entries now pointed to HTTP 410 ‚ÄúResource deleted‚Äù responses ‚Äî a code that explicitly signals permanent removal to search engines.\nIn other words, for several days, OSF‚Äôs infrastructure told the world that our work no longer existed."
  },
  {
    "objectID": "posts/2025/open-science-needs-reliable-infrastructure/index.html#the-deeper-problem",
    "href": "posts/2025/open-science-needs-reliable-infrastructure/index.html#the-deeper-problem",
    "title": "Open Science needs reliable infrastructure",
    "section": "The Deeper Problem",
    "text": "The Deeper Problem\nThis incident isn‚Äôt about data loss ‚Äî most of my materials were mirrored elsewhere. The issue is trust.\nResearchers use OSF because it promises persistence. It issues DOIs, integrates with journals, and serves as part of the scholarly record. That promise was violated ‚Äî not through malice, but through a structural design choice: an automated filter was allowed to silently deindex archival content.\nSpam detection is necessary. But a true archive must never delete first and explain later. An archive is defined by its immutability and accountability.\nThe design flaw wasn‚Äôt that a spam flag existed ‚Äî it‚Äôs that its activation could unpublish a decade‚Äôs worth of DOI-assigned material without human review, notification, or even the retention of metadata.\nThis episode highlights a broader fragility in the open science ecosystem. The infrastructure of modern scholarship ‚Äî repositories, indexing services, DOI registries ‚Äî runs on trust. When that infrastructure silently fails, the damage is epistemic: we lose not just access to data, but confidence in the permanence of the scientific record.\nAn archival platform must therefore:\n\nRetain metadata and landing pages even when content is quarantined.\nNotify users when materials are flagged or hidden\nRequire human verification before suppressing DOI-linked material.\nUse non-destructive HTTP codes for temporary removals.\n\nThese are not minor UX fixes ‚Äî they are the foundations of reliability in digital scholarship. I made this much clear in an email reply:\n\n&lt;On Mon, Nov 3, 2025 at 2:26‚ÄØPM Ven Popov wrote:&gt;\nThank you for resolving the issue and restoring access to my materials. I understand that OSF, as a free and open platform, must protect itself against spam.\nHowever, the way your spam-filtering system currently operates is unacceptable and incompatible with OSF‚Äôs stated mission as an archival repository. Automatically blocking access to more than twenty DOI-registered preprints, datasets, and supplementary materials spanning nearly a decade ‚Äî including work co-authored by dozens of researchers and cited in published papers ‚Äî is not a minor glitch. It constitutes a violation of the very FAIR principles OSF promotes and undermines trust in your platform as part of the scholarly record.\nWorse, the system suppressed all metadata and landing pages, returning an HTTP 410 ‚ÄúResource deleted‚Äù response. That code explicitly signals to search engines that a resource has been permanently removed and should be de-indexed. This means the issue not only disrupted access but actively damaged the discoverability and citation continuity of legitimate research outputs.\nI fully support the need for robust spam protection. But the current implementation ‚Äî silent removal of legitimate archival content, without notice or human verification, and with a response that signals permanent deletion ‚Äî is deeply inconsistent with COS‚Äôs commitments to openness, transparency, and persistence.\nI urge COS to review this policy and implement safeguards such as:\n\nHuman review before suppressing any DOI-assigned or public scholarly record.\nRetention of metadata and landing pages for any quarantined content.\nClear notifications to affected users.\nUse of non-destructive HTTP status codes (e.g., 451 or 503) for temporary suppression.\n\nThese are not just usability issues; they concern the credibility of OSF as a reliable component of the scientific infrastructure.\nI am deeply troubled by this incident, and I do not say this lightly. I have been a staunch supporter of COS and its mission for years. But this event exposes an infrastructural weakness that seriously undermines trust in the OSF platform.\nI would appreciate a response from COS leadership that demonstrates an understanding of the gravity of this failure and provides a clear plan to prevent similar occurrences in the future.\nI intend to publish this correspondence publicly, and I hope that I can do so alongside a response that acknowledges these concerns and reaffirms COS‚Äôs commitment to openness, transparency, and durability in scientific communication."
  },
  {
    "objectID": "posts/2025/open-science-needs-reliable-infrastructure/index.html#aftermath",
    "href": "posts/2025/open-science-needs-reliable-infrastructure/index.html#aftermath",
    "title": "Open Science needs reliable infrastructure",
    "section": "Aftermath",
    "text": "Aftermath\nAfter I sent my detailed critique, Brian Nosek responded personally and with grace. He acknowledged that this case was unprecedented, thanked me for the thorough documentation, and assured me that it would help the team identify whether others had been similarly affected.\n\n&lt; On Nov 3, 2025 at 22:05 +0100, Brian Nosek, wrote:&gt;\nI see from the¬†thread that your issue is resolved, but I am sorry that you had to go through that!\nI really appreciate the detailed notes that you provided to help identify the source of the problem, and even more the solutioning that you offered to improve user experience and more gracefully manage the identification and mitigation of spam. I have not heard of a case like this, so while it was a painful experience that you had to deal with, hopefully the exposure of the problem with your diligent reporting will help us to identify the extent to which it is occurring more widely and develop effective solutions.\nThanks again for giving us such useful reporting on your experience with this to help us continue to improve the service.\n\nDespite this response, I was still deeply troubled. I briefly considered whether I‚Äôm overreacting. After consulting several colleagues, they were unanimous: the severity is not that it happened to me, but that it could happen at all ‚Äî silently and retroactively. As one colleague put it:\n\nI agree, I think the issue is more severe than it sounds like. The effect is crazy to me. Not only that they retrospectively delete the stuff but also how many people can be affected by it. It‚Äôs already severe in your case with a large network of collaborators. But imagine Klaus‚Äôs account gets flagged and he doesn‚Äôt realize it. This would probably take hundreds of preprints offline, which are primary the work of others [junior researchers]. It‚Äôs bad enough that they take the stuff down that you put on OSF but it also deletes the work where someone else just linked your name with? This also can‚Äôt be right‚Ä¶\n\nAfter some reflection, I followed up with Brian Nosek with a longer message explaining why the issue mattered so deeply: not because of personal inconvenience, but because silent retroactive suppression of long-standing records is incompatible with the role of an archive.\n\n&lt; On Nov 4, 2025 at 13:49 +0100, Ven Popov wrote:&gt;\nThank you for the thoughtful response. I appreciate that you will take this case seriously in developing future solutions.\nI just really want to stress why I am so concerned about this having happened. It‚Äôs not about my work and that it affected me personally, although that clearly made me more motivated to get to the bottom of it. My work is also backed up on other platforms, including ResearchGate, a University of Zurich repository and my personal website. The real problem is that something like this should not be possible to happen in the first place, let alone as the result of an automated filter.\nWhatever the reason my account was flagged, an automated filter should never have the authority to retroactively block access to the work of hundreds of researchers simply because I‚Äôm listed as a co-author. These were not new uploads but materials that had been part of the scholarly record for nearly a decade.\nI was lucky to notice the disappearance within days; others might not. Under different circumstances, months could have passed during which all these digital objects appeared effectively erased. The consequences for more prominent researchers with hundreds of linked projects - and for junior collaborators relying on those links - could be far more severe.\nYour own guidelines stress that even authors themselves cannot delete accepted preprints - that preprints can only be withdrawn, and that the associated metadata and reasons for withdrawn will always remain part of the record.\nI considered whether I might be overreacting, but after discussing the issue with several colleagues, everyone agrees that this is a critical infrastructure-level bug. This requires more than a patch for this particular case, but systems in place to ensure that nothing can in principle cause something like it to happen in the future.\nI am writing all this out in detail not to attack COS or its work on maintaining OSF. It‚Äôs exactly the opposite - I share all COS values and view it as a fundamentally important institution. The traditional academic publishing system is rotten to core, but in order for any reform to succeed long-term, people must trust the infrastructure and systems in place. I want to see COS and OSF continue to succeed and continue to play the vital role that it has attained. This is why I am being so adamant about this incident being taken so seriously.\n\nHis reply was thoughtful and for the moment I am satisfied that this incident will be taken seriously:\n\n&lt;On Nov 4, 2025 at 14:45 +0100, Brian Nosek, wrote:&gt;\nThanks for the follow-up. There is nothing inappropriate at all about your comments or approach. It is very clear that your comments are made in good faith to help us improve the services and meet the aspirations of reliable, persistent open scholarship. We truly appreciate your willingness to put time into documenting and sharing the challenges you experienced here.\n\nThat, to me, felt like the right place to stop. Ideally, I‚Äôd still like an official statement from COS about what they plan to do to ensure that something like this doesn‚Äôt happen again."
  },
  {
    "objectID": "posts/2025/open-science-needs-reliable-infrastructure/index.html#data-vs.-trust",
    "href": "posts/2025/open-science-needs-reliable-infrastructure/index.html#data-vs.-trust",
    "title": "Open Science needs reliable infrastructure",
    "section": "Data vs.¬†Trust",
    "text": "Data vs.¬†Trust\nNo data were ultimately lost. But something far more important was at stake: the continuity of the scholarly record.\nIf this episode serves any purpose, let it be a reminder that openness is not just about making information accessible ‚Äî it‚Äôs about ensuring it stays accessible. Scientific infrastructure must be engineered for trustworthiness, not just availability.\nAs we argued in a recent perspective paper, the traditional publishing system is rotten to the core. But for any reform to succeed long-term, people must trust the infrastructure and systems in place.\nI also don‚Äôt know if this incident is related to the OSF redesign - I described it here within that context - or whether it would have occurred with the previous platform. Ultimately it doesn‚Äôt matter.\nTrust is slow to earn and quick to erode. I hope OSF uses this experience to reinforce that trust ‚Äî not only by fixing the bug, but by institutionalizing the safeguards that make open scholarship durable."
  },
  {
    "objectID": "posts/2024/taking-a-stand-on-open-peer-review/index.html",
    "href": "posts/2024/taking-a-stand-on-open-peer-review/index.html",
    "title": "Taking a stand on open peer review",
    "section": "",
    "text": "I have spent the last few months thinking about academic publishing and the peer review system. The whole thing is rotten and no sane person would design the system we have today. Now that I find myself in a professional position where I don‚Äôt have to worry about my future, I realize I can take actions to help dismantle this system. In other words, I can finally afford to act on my principles (as absurd as that sounds).\nI am not yet ready to share the full extent of what this entails, and I plan a longer post about the future of academic publishing. But last week was the first time I took action consistent with these principles, specifically my belief that all peer review should be open, published alongside the work it reviews. I received a standard review request from a respected APA journal, and this was my response:\n\nDear XXX,\nThank you for inviting me to review this submission. I would be glad to review the manuscript if my review and the subsequent author responses are published alongside the paper, should it be accepted.\nI understand this is not the current practice at XXX, but I firmly believe that open review is the future of academic publishing. It is a responsibility we owe to the public that funds our research. This is not about receiving credit for my review work but about promoting transparency and accountability in the peer review process. Many modern publishing platforms already support this practice.\nI appreciate that finding reviewers is challenging and that this request might make your job as an editor more difficult.\nBest regards,\nVen\n\nWill this have any effect? I doubt it. The system is too rigid. I expect that the editor would ask me to reconsider, and move on to other reviewers. Which is unfortunate, because I really don‚Äôt want to add more burden to a difficult job. But change has to start from somewhere.\nUpdate: As I suspected, the Editor sent me a polite email saying that the journal currently does not allow this, but that they will keep reviewing their policies as practices in the field change.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{popov2024,\n  author = {Popov, Vencislav},\n  title = {Taking a Stand on Open Peer Review},\n  date = {2024-08-04},\n  url = {https://venpopov.com/posts/2024/taking-a-stand-on-open-peer-review/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPopov, Vencislav. 2024. ‚ÄúTaking a Stand on Open Peer\nReview.‚Äù August 4, 2024. https://venpopov.com/posts/2024/taking-a-stand-on-open-peer-review/."
  },
  {
    "objectID": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html",
    "href": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html",
    "title": "Reduce friction for creating Quarto blog posts",
    "section": "",
    "text": "My relationship with blogging is complicated. I made my first blog about 15 years ago, fresh out of high school. WordPress was the dominant platform at the time, and my blog wasn‚Äôt anything particularly interesting - I shared some vapid attempts at poetry and short story writing, some travel logs, and a mish-mash of other topics1.\nI have never liked WordPress, for mostly the same reasons that I don‚Äôt like most software - I found every action incredibly cumbersome. Shortly thereafter I joined Facebook and Twitter, two platforms I no longer use, and most of my writing took on the ‚Äúmicro-blogging‚Äù nature that social media lends itself to so easily. I‚Äôve always suspected that one of the reasons social media blew up was that it made it so easy to write, share, and follow content. At some point during my PhD (circa 2017), I created another WordPress blog, out of a desire to write longer pieces on more technical topics. That blog was again short-lived and the reason was simple - I wanted to write about programming, data analysis, and R, to share computational insights, but WordPress was a poor medium for it. At the time I was just starting to get enamored with literate programming (Jupyter notebooks were hot off the press), and the ease with which I could create computational writing contrasted starkly with the difficulty of sharing such writing on the internet.\nLast year I discovered Quarto and I immediately became excited by the possibilities. I rebuilt my personal website with it and set up this blog to go with it. I expected to post a lot more often than I have done in reality. There are many reasons for this - from starting a new tenured position, to being a typical overworked, overextended, and overcommitted academic.\nWhile I can‚Äôt do anything about that, I realized that one mental barrier arose every time I considered making a new post. Despite how easy Quarto makes it to publish technical material, there are a number of steps, at least in the way I have currently set up my system, that are just busywork, which I have to do any time I want to create a new post:\nFor some reason, the first three steps, as small as they appear, were a mental barrier that stopped me from even beginning new posts. Ideally, I want this to be as simple as possible. So I decided to write a bash script that automates it."
  },
  {
    "objectID": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html#a-bash-script-for-automating-post-creation",
    "href": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html#a-bash-script-for-automating-post-creation",
    "title": "Reduce friction for creating Quarto blog posts",
    "section": "A bash script for automating post creation",
    "text": "A bash script for automating post creation\nThe problem was that aside from some basic commands, my bash knowledge is quite limited. So I figured this was an opportunity to put Github Copilot to the test. I use Visual Studio Code as my IDE, and have setup my website repository as a workspace. I already had a markdown document that describes in plain language the steps I need to do when creating new blog posts2. So could the built-in Copilot in VSCode write me a bash script just by being told to implement the steps I‚Äôve described in my little instruction manual?\nIt worked flawlessly, and this code was the result after a few iterations:\n#!/bin/bash\n\n# Prompt for post details\nread -p \"Enter post title: \" title\nread -p \"Enter post subtitle: \" subtitle\nread -p \"Enter post categories (comma-separated): \" categories\n\n# Convert title to a slug for the folder name\nslug=$(echo \"$title\" | tr '[:upper:]' '[:lower:]' | tr -cd '[:alnum:] ' | tr ' ' '-')\n\n# Create the new post directory\nyear=$(date +%Y)\npost_dir=\"$HOME/venpopov.com/posts/$year/$slug\"\n\n# Check if the directory already exists\nif [ -d \"$post_dir\" ]; then\n  echo \"Error: The directory $post_dir already exists.\"\n  exit 1\nfi\n\nmkdir -p \"$post_dir\"\n\n# Get today's date\ndate=$(date +%Y-%m-%d)\n\n# Append the current year to the categories list\ncategories=\"$categories, $year\"\n\n# Create the .qmd file with the provided metadata\ncat &lt;&lt;EOF &gt; \"$post_dir/index.qmd\"\n---\ntitle: \"$title\"\nsubtitle: \"$subtitle\"\ncategories: [$(echo \"$categories\" | sed 's/,/, /g')]\ndate: \"$date\"\n---\nEOF\n\necho \"New post created at $post_dir/index.qmd\"\n\n# Open the new post in the default editor\ncode \"$post_dir/index.qmd\"\nI saved this code in a file new_blog_post and Copilot instructed me that I can make it executable from the command line by first setting file permission via:\nchmod +x new_blog_post\nI saved this file in a directory on my path, so now when I want to create a new blog post, I simply open a terminal and type\nnew_blog_post\nThis prompts me to enter a title for the post, a subtitle and some tags, then creates all the necessary boilerplate and opens the file in VSCode for editing. In fact, I used it to make this very post, and for example, here is the terminal output from it\n\nNeat. Will this help me post more often? Time will tell."
  },
  {
    "objectID": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html#postscript-ai-and-learning-to-code",
    "href": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html#postscript-ai-and-learning-to-code",
    "title": "Reduce friction for creating Quarto blog posts",
    "section": "Postscript: AI and learning to code",
    "text": "Postscript: AI and learning to code\nI tell my students that they can use AI tools when working on projects, but I urge them that it is still important to learn how to code for many reasons. They need to understand what the code is doing, at the very least, and know how to fix it. Did I follow my own advice here? Ugh, maybe not, depending on the perspective. I still have no idea about the core bash syntax and how to write a script from scratch. But I do have a ‚Äúlifetime‚Äù of experience with programming, and I could understand what the script was doing, and I could modify it to suit my needs. I think that‚Äôs the important part. I don‚Äôt need to know everything about bash scripting, but I need to know enough to be able to use it effectively. In any case, I have a tendency to fall into rabbit holes when learning new things, and this is the last thing I want to do right now while trying to optimize my workflow! So, I have a working script, I asked Copilot to add some reasonable checks that occurred to me, and I know what each line does, even if I don‚Äôt know how to write it from scratch. I think that‚Äôs good enough for now."
  },
  {
    "objectID": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html#footnotes",
    "href": "posts/2024/reduce-friction-for-creating-quarto-blog-posts/index.html#footnotes",
    "title": "Reduce friction for creating Quarto blog posts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo my huge surprise, the blog still exists! The last entry is from 2013, and I hadn‚Äôt looked at it probably since then. I only rediscovered it now while writing this post.‚Ü©Ô∏é\nI write these types of ‚ÄúNotes to self‚Äù instruction manuals for many things, because my memory is very poor for procedural operations‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2024/introducing-bmm/index.html",
    "href": "posts/2024/introducing-bmm/index.html",
    "title": "Introducing the Bayesian Measurement Modeling R Package (bmm)",
    "section": "",
    "text": "I am excited to announce the first official release of the Bayesian Measurement Modeling R package! bmm makes Bayesian measurement modeling in psychology accessible to all. Over the past two years, Gidon Frischkorn and I worked closely together on this package and we are thrilled to finally share it with the world.\nAside from the final result, this collaboration was the most fun I‚Äôve had doing research in a long time. I stopped counting how many days we started a random unplanned coffee chat and to suddenly realize that it‚Äôs been three hours of intense problem-solving, only because one of us was actually late for a real meeting. bmm is truly both of our baby (only slightly older that Gidon‚Äôs soon-to-be second flesh-and-blood daughter!) and we hope it will make fitting Bayesian measurement models easier, more reliable, and more efficient for everyone.\nYou can find detailed documentation, tutorials and examples on the package website. You can install bmm from CRAN as follows:"
  },
  {
    "objectID": "posts/2024/introducing-bmm/index.html#what-is-bmm-for",
    "href": "posts/2024/introducing-bmm/index.html#what-is-bmm-for",
    "title": "Introducing the Bayesian Measurement Modeling R Package (bmm)",
    "section": "What is bmm for?",
    "text": "What is bmm for?\nbmm provides a simple and intuitive interface for fitting measurement models in psychology using Bayesian methods. It is designed to be accessible to researchers with little to no experience with Bayesian statistics or cognitive modeling. If you know how to fit a mixed-effects regression model in R, you already know nearly everything you need to fit a measurement model with bmm.\n\n\n\n\n\nBut first things first: what are measurement models and why do we need them?\nIf you are a meteorologist, you can measure atmospheric temperature with a thermometer. And while thermometers are technically indirect measures, the relationship between the volume of a liquid and its temperature is so strong and so well understood that we take it for granted.\nIf you are a psychologist, things get a bit more complicated.\nBehavioral and psychological data are messy and only rough proxies for what they are supposed to measure. We rarely care about how much time it takes someone to press a button after a flashing light, how precisely they can remember a specific shade of blue, or how similar they judge two badly drawn alien animals to be. Such data are only interesting insofar as they tell us something about the underlying psychological processes that govern attention, perception, decision making, memory and categorization.\nTo make matters even more difficult, usually multiple distinct cognitive processes contribute to behavior. Combined, these issues often make it difficult to draw clear conclusions from behavioral data alone. Running t.tests on averaged raw behavioral data can only get you so far. Measurement models are an important tool to bridge the gap between latent psychological constructs and observable behavioral data.\nSuch measurement models are nowadays used in many different areas of psychology. Some of the more popular models include drift diffusion models in decision making, signal detection theory models in perception and memory, and mixture models in visual working memory. The basic idea uniting these approaches is that we can decompose one or more observed measures (e.g., reaction times, accuracy rates, angular deviation, confidence ratings) into distinct theoretically meaningful parameters that reflect latent psychological processes (e.g., decision thresholds, different memory strength signals, the quality or precision of representations). These derived parameters can then be used to test hypotheses about the underlying cognitive processes that govern behavior."
  },
  {
    "objectID": "posts/2024/introducing-bmm/index.html#the-challenges-of-measurement-modeling",
    "href": "posts/2024/introducing-bmm/index.html#the-challenges-of-measurement-modeling",
    "title": "Introducing the Bayesian Measurement Modeling R Package (bmm)",
    "section": "The challenges of measurement modeling",
    "text": "The challenges of measurement modeling\n\n\n\nUnfortunately, technical challenges have prevented widespread adoption of measurement models for data analysis in psychology. And even when researchers do use these models, many (*cough *) rely on ad-hoc custom implementations that are not well-documented, not well-tested, not well-understood, not easily portable to new experiments, and not using optimal inference methods. This is where bmm comes in.\n\n\n\n\n\n\nTraditionally, to fit a measurement model in psychology, you had to build it yourself. The simple reality is, however, that most researchers don‚Äôt have the time, resources, or expertise to build and fit these models from scratch. This is doubly true for Bayesian hierarchical implementations, which require a different set of tools and skills than traditional maximum likelihood estimation. And even for those who do have the skills, the process can be time-consuming and error-prone.\n\n\n\n\n\nThis is why we started working on the bmm package in the first place - we were tired of doing the same thing over and over again for every new project. What started as a personal project to make our own daily work easier has now turned into a fully-fledged R package. What we have built is a package that allows you to fit a wide range of measurement models with just a few lines of code.\nYou no longer need to:\n\ncopy-paste custom Jags or Stan code from one project to the next (or worse, try to decipher someone else‚Äôs code!) and painstakingly adjust it for your new experiment\nworry about whether you have correctly specified your priors or likelihoods\nworry about whether you have correctly implemented your model\nworry about how to adjust the script for your new experimental design and to spend hours debugging it\n\nThe bmm package takes care of all of that for you.\nWhile there exist tools for some measurement models, such as for drift diffusion models (via the wiener distribution in brms for R or via the HDDM package for Python), this is not the case for the vast majority of measurement models used in psychology. The bmm package aims to fill this gap by providing a general framework that can be extended and continuously improved by the community."
  },
  {
    "objectID": "posts/2024/introducing-bmm/index.html#example---the-interference-measurement-model",
    "href": "posts/2024/introducing-bmm/index.html#example---the-interference-measurement-model",
    "title": "Introducing the Bayesian Measurement Modeling R Package (bmm)",
    "section": "Example - the Interference Measurement Model",
    "text": "Example - the Interference Measurement Model\nTo give you a sense of how easy it is to fit a model using the bmm package, let‚Äôs walk through an example. Let‚Äôs say you have collected data from a continuous reproduction visual working memory task (see the margin for a visual representation of the task and the distribution of the data).\n\n\n\n\n\nParticipants study 1 to 8 colors in different locations. After a delay, they reproduce one of the colors as precisely as they can on a color wheel. Click image to enlarge.\n\n\n\n\n\n\n\nDensity plots of the deviation from the target color in radians, split by set size. Click image to enlarge.\n\n\n\n\nAssume you want to fit the Interference Measurement Model (IMM)(Oberauer et al. 2017) to your data. For convenience, we have included the data from this study in the bmm package, so we can use it directly and fit the model to it. The IMM model attributes errors to different sources of activation in memory: target features, non-target features, the spacial distance between them, and random noise1.\nBefore we go into the different parts, here is the entire code that you would need to fit the IMM model to the data. Yes, it‚Äôs that simple:\nlibrary(bmm)\n\n# load the data\nmy_data &lt;- oberauer_lin_2017\n\n# inform the model about the data structure\nimm_model &lt;- imm(\n  resp_error = \"dev_rad\",\n  nt_features = \"col_nt\",\n  nt_distances = \"dist_nt\",\n  set_size = \"set_size\",\n  version = \"full\",\n  regex = TRUE\n)\n\n# specify the regression formula for the model parameters\nimm_formula &lt;- bmmformula(\n  c ~ 0 + set_size + (0 + set_size | ID),\n  a ~ 0 + set_size + (0 + set_size | ID),\n  s ~ 0 + set_size + (0 + set_size | ID),\n  kappa ~ 0 + set_size + (0 + set_size | ID)\n)\n\n# fit the model via `brms` and `Stan`\nimm_fit &lt;- bmm(\n  formula = imm_formula,\n  data = my_data,\n  model = imm_model,\n  cores = 4\n)\nLet‚Äôs take a brief look at the data we are working with. In this case, the dependent variable is dev_rad - the deviation of the response from the target color (in radians). col_nt1 to col_nt7are the colors of the non-target items which were studied but are not being tested (coded relative to the target). Finally, dist_nt1 to dist_nt7 are the spatial distances of the non-target items from the target item, and set_size is the number of items in the display.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nsession\ntrial\nset_size\ndev_rad\ncol_nt1\ncol_nt2\ncol_nt3\ncol_nt4\ncol_nt5\ncol_nt6\ncol_nt7\ndist_nt1\ndist_nt2\ndist_nt3\ndist_nt4\ndist_nt5\ndist_nt6\ndist_nt7\n\n\n\n\n1\n1\n1\n7\n0.3839724\n0.8726646\n0.8552113\n2.7750735\n2.094395\n1.2566371\n0.0698132\nNA\n1.9332878\n2.416610\n1.9332878\n0.4833219\n0.9666439\n2.899932\nNA\n\n\n1\n1\n2\n3\n-0.4537856\n0.8552113\n1.9547688\nNA\nNA\nNA\nNA\nNA\n1.9332878\n2.416610\nNA\nNA\nNA\nNA\nNA\n\n\n1\n1\n3\n5\n-0.0872665\n0.7155850\n-2.6179939\n-1.0297443\n1.378810\nNA\nNA\nNA\n0.9666439\n2.899932\n2.4166097\n1.9332878\nNA\nNA\nNA\n\n\n1\n1\n4\n6\n0.3665191\n0.2617994\n2.0420352\n0.1047198\n1.099557\n-0.9250245\nNA\nNA\n0.4833219\n2.899932\n0.9666439\n0.9666439\n1.9332878\nNA\nNA\n\n\n1\n1\n5\n1\n-0.0349066\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n1\n1\n6\n1\n0.1396263\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nTo fit the IMM model to this data, we need to follow a few simple steps:\n\n1) Specify the model\nFirst, set up the bmmodel object to specify which variables in your data contain information about the identity of the target and non-target features, the distances between them, and the set size of the display. For the IMM model, this would look like this:\n\nimm_model &lt;- imm(\n  resp_error = \"dev_rad\",\n  nt_features = \"col_nt\",\n  nt_distances = \"dist_nt\",\n  set_size = \"set_size\",\n  version = \"full\",\n  regex = TRUE\n)\n\nHere we used the regex option to specify that columns which begin with col_nt and dist_nt should be treated as non-target features and distances, respectively. This is useful when you have multiple non-target features and distances in your data (instead of enumerating them all).\n\n\n2) Specify the regression formula for the model parameters\nSecond, specify how the parameters of the model should vary over different conditions. A list of all model parameters and their meaning is saved in the model object and can be accessed using the parameters element.\n\nimm_model$parameters\n#&gt; $mu1\n#&gt; Location parameter of the von Mises distribution for memory responses (in radians). Fixed internally to 0 by default.\n#&gt; \n#&gt; $kappa\n#&gt; [1] \"Concentration parameter of the von Mises distribution\"\n#&gt; \n#&gt; $a\n#&gt; [1] \"General activation of memory items\"\n#&gt; \n#&gt; $c\n#&gt; [1] \"Context activation\"\n#&gt; \n#&gt; $s\n#&gt; [1] \"Spatial similarity gradient\"\n\nUsing this information, we can set up the bmmformula for the different model parameters. Let‚Äôs say we want to first get an idea how all parameters vary across set_size:\n\nimm_formula &lt;- bmmformula(\n  c ~ 0 + set_size + (0 + set_size | ID),\n  a ~ 0 + set_size + (0 + set_size | ID),\n  s ~ 0 + set_size + (0 + set_size | ID),\n  kappa ~ 0 + set_size + (0 + set_size | ID)\n)\n\nThe bmmformula object is closely aligned with the brmsformula syntax and allows for an easy specification of grouped effects. In this case, we assume random effects for all parameters over the ID variable, thus implementing a hierarchical model estimating individual differences in all parameters across the different set sizes.\n\n\n\n\n\n\nTipTake a closer look at the formula object.\n\n\n\nThis is where the power and flexibility of this approach comes from. You can specify any combination of fixed and random effects for any parameter in the model. This allows you to test a wide range of hypotheses about how the parameters of the model vary across different experimental conditions. In essence, fitting a hierarchical measurement model is not much different from fitting a mixed-effects regression model. You can find a more comprehensive tutorial on the bmmformula syntax and features in the online vignette\n\n\n\n\n3) Fit the model\nFinally, we only need to call the bmm function to estimate the model. For this, we pass the data the specified bmmodel and bmmformula to the function. In addition, we can pass additional options to the function to customize sampling (warmup, iter, chains, cores), save the fitted model object (file), or choose the backend the model should be estimated with.\nimm_fit &lt;- bmm(\n  # required inputs\n  data = my_data,\n  model = imm_model,\n  formula = imm_formula,\n  \n  # customize sampler settings\n  warmup = 1000,\n  iter = 2000,\n  chains = 4,\n  cores = 4,\n  \n  # save fitted model object\n  file = \"imm_fit\"\n)\nThe bmm package is closely integrated with brms, the leading R package for Bayesian Regression Models. This allows you to use almost any post-processing and inference method implemented for brms models with the measurement models implemented in bmm\nYou can find more detailed introductions into the different model currently implemented in bmm on the package website. And we have also written a tutorial paper that explains more details about the implementation of several measurement models, and how to specify bmmodels in different experimental settings."
  },
  {
    "objectID": "posts/2024/introducing-bmm/index.html#design-principles",
    "href": "posts/2024/introducing-bmm/index.html#design-principles",
    "title": "Introducing the Bayesian Measurement Modeling R Package (bmm)",
    "section": "Design principles",
    "text": "Design principles\nThe bmm package is built on a few key design principles, some of which were strongly inspired by the brms package for Bayesian regression modeling:\n\nSimplicity\nFitting a cognitive measurement model should be as simple as fitting a linear, logistic or a poisson regression model. You select the model you want to fit, specify a formula to predict how each parameter of the model varies across different experimental conditions, and hit go\n\n\nFlexibility\nthe package should be able to handle a wide range of measurement models and experimental designs. Any model for which you can write down a likelihood function should be able to be fit with bmm. And every parameter in the model should be able to vary across any combination of continuous and categorical predictors. This is a big departure from many existing techniques, which fit models separately to each experimental condition, forcing all parameters to vary across the same set of conditions, and preventing you from including continuous predictors. You should also be able to fix some parameters to specific values, or to impose complex constraints on them\n\n\nHierarhical estimation and one step inference\nRather than fitting each model separately to each participant, the package should allow you to estimate all parameters of the model simultaneously, while accounting for individual differences and benefit from shrinkage and sharing of information. As we detail in our tutorial paper, this has a number of advantages, including more stable estimates of population parameters, better generalization to new data, and more reliable estimates of individual differences (especially when there are few trials per participant)\n\n\nReliability and documentation\nAll models should be thoroughly tested and documented, so you can be confident that the model you are fitting is the model you think you are fitting. Each model should come with references to the relevant literature, so you can understand the theoretical background of the model and how it is implemented in bmm. The package should also provide detailed information on how to specify the model, including which parameters are available and how they can be varied across different experimental conditions\n\n\nParameter recovery\nRelated to the previous point, the package should be able to recover the parameters of the model from simulated data. These parameter recovery studies should be available as online vignettes and be fully reproducible, rather than being buried in the supplementary materials of academic papers (currently work in progress). Any trade-offs in the parameter recovery should be clearly documented, so you can understand the limitations of the model and how to interpret the results.\n\n\nEfficiency and future-proofing\nUse state-of-the-art sampling algorithms and optimization techniques to ensure that the models can be fit quickly and accurately. bmm is built on top of the brms package, which itself is an interface to the Stan probabilistic programming language. This means that the models are fit using the No-U-Turn Sampler (NUTS) algorithm, which is a state-of-the-art Hamiltonian Monte Carlo algorithm. These are packages that are actively maintained, used and supported by a large community of researchers, so that any future advances in the field of Bayesian statistics can be easily incorporated into the package as alternative backends or sampling algorithms. Furthermore, the package should be designed in a modular way, so that new models can be easily added by the community without having to change the core codebase. Finally, we have been working actively with the core developers of brms and Stan to improve sampling speed and stability of existing distributions used by our models."
  },
  {
    "objectID": "posts/2024/introducing-bmm/index.html#currently-supported-models",
    "href": "posts/2024/introducing-bmm/index.html#currently-supported-models",
    "title": "Introducing the Bayesian Measurement Modeling R Package (bmm)",
    "section": "Currently supported models",
    "text": "Currently supported models\nWe currently have the following models implemented:\nVisual working memory\n\nInterference measurement model by Oberauer and Lin (2017).\nTwo-parameter mixture model by Zhang and Luck (2008).\nThree-parameter mixture model by Bays et al (2009).\nSignal Discrimination Model (SDM) by Oberauer (2023)\n\nHowever, the setup of the bmm package provides the foundation for the implementation of a broad range of cognitive measurement models. In fact, we are already working on implementing additional models, such as:\n\nSignal-Detection Models\nEvidence Accumulation Models\nMemory Models for categorical response\n\nIf you have suggestions for models that should be added to the package, feel free to create an issue on GitHub. Ideally this should describe the model, point towards literature that gives details on the model, and if possible link to code that has already implemented the model.\nGiven the dynamic nature the bmm package is currently in, you can always view the latest list of supported models by running:\nbmm::supported_models()\nSo stay tuned for updates and new models! We hope you will find the bmm package useful and will try fitting one of the already available models to your data. We appreciate all feedback and hope that the bmm package will make the use of measurement models easier for everybody."
  },
  {
    "objectID": "posts/2024/introducing-bmm/index.html#footnotes",
    "href": "posts/2024/introducing-bmm/index.html#footnotes",
    "title": "Introducing the Bayesian Measurement Modeling R Package (bmm)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe won‚Äôt go into much detail here - you can find a detailed explanation of the IMM model and results of the model fitting in the package documentation.‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me",
    "section": "",
    "text": "Github\n  \n  \n      Email\n  \n  \n      ORCID 0000-0002-8073-4199\n  \n  \n      Google Scholar\n  \n\n  \n  \n\nI am a tenured Senior Scientist in Computational Modeling of Behavior at the Department of Psychology, University of Z√ºrich. I completed my PhD in Cognitive Psychology from Carnegie Mellon University under Prof.¬†Lynne Reder in 2020, and until 2023 I was a postdoctoral researcher with Prof.¬†Klaus Oberauer at the University of Z√ºrich. My research has been awarded the 2025 Bertelson Early Career Award from the European Society for Cognitive Psychology and the 2021 Glushko Dissertation Prize from the Cognitive Science Society. I am also a consulting editor at the Memory & Cognition.\nI use computational modeling to understand the structure, organization, and function of fundamental cognitive processes, to refine the development and evaluation of psychological theories, and to improve psychological measurement.\nI publish under my full name, Vencislav Popov, but Ven is easier to say and remember, and as people have noted, ‚ÄúOh, Ven, like a Venn diagram!‚Äù\n\n\nMy current research focus is on how to improve inference in psychological science to resolve the theory crisis. To that aim I develop mechanistic models that specify theoretical assumptions about how psychological processes produce behavior. So far I have done that mostly in my own field of human memory to understand how limited cognitive resources affect memory strength, but I am eager to extend such approaches to other non-cognitive phenomena. To strengthen the evaluation of theoretical models, I develop techniques for improving psychological measurement and new standards for model evaluation. To make measurement models accessible for everyone, I develop flexible and easy to use hierarchical Bayesian implementations in R. I am committed to open science, and I am a strong advocate for Bayesian inference.\nSeveral goals drive my research program, including:\n\ndeveloping comprehensive computational models of human memory\nunderstanding how people control limited cognitive resources\ndeveloping and evaluating new methods for measuring cognitive processes\ndeveloping new methods and tools for computational modeling of behavior\napplying computational modeling in non-cognitive areas of psychology\nmeasuring theoretical knowledge accumulation in psychology\naddressing the theory crisis in psychology"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "About me",
    "section": "",
    "text": "My current research focus is on how to improve inference in psychological science to resolve the theory crisis. To that aim I develop mechanistic models that specify theoretical assumptions about how psychological processes produce behavior. So far I have done that mostly in my own field of human memory to understand how limited cognitive resources affect memory strength, but I am eager to extend such approaches to other non-cognitive phenomena. To strengthen the evaluation of theoretical models, I develop techniques for improving psychological measurement and new standards for model evaluation. To make measurement models accessible for everyone, I develop flexible and easy to use hierarchical Bayesian implementations in R. I am committed to open science, and I am a strong advocate for Bayesian inference.\nSeveral goals drive my research program, including:\n\ndeveloping comprehensive computational models of human memory\nunderstanding how people control limited cognitive resources\ndeveloping and evaluating new methods for measuring cognitive processes\ndeveloping new methods and tools for computational modeling of behavior\napplying computational modeling in non-cognitive areas of psychology\nmeasuring theoretical knowledge accumulation in psychology\naddressing the theory crisis in psychology"
  },
  {
    "objectID": "index.html#research-program",
    "href": "index.html#research-program",
    "title": "About me",
    "section": "Research program",
    "text": "Research program"
  },
  {
    "objectID": "CV/index.html",
    "href": "CV/index.html",
    "title": "Curriculum vit√¶",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Computational thinking",
    "section": "",
    "text": "Simple rules, hard problems and the emergence of meaning in mathematics\n\n\n\nmath\n\nphilosophy\n\nemergence\n\nessay\n\nmeaning\n\nstructure\n\n2026\n\n\n\nWhat are mathematical objects? And what does it mean to understand something that by some accounts doesn‚Äôt really exist?\n\n\n\n\n\nFeb 11, 2026\n\n13 min\n\n\n\n\n\n\nPackage management and reproducibility are a nightmare\n\n\n\nR\n\nR package\n\n2026\n\npackage management\n\nreproducibility\n\nworkflow\n\n2026\n\n\n\nI continue my love-hate relationship with R, renv and Quarto\n\n\n\n\n\nFeb 3, 2026\n\n4 min\n\n\n\n\n\n\nOpen Science needs reliable infrastructure\n\n\n\nopen science\n\nreproducibility\n\nacademic publishing\n\n2025\n\n\n\nAfter OSF‚Äôs October 2025 redesign, I discovered that eight years of DOI-linked preprints and materials were silently hidden by an automated spam flag. What happened, how it was resolved, and what it reveals about trust in open science infrastructure.\n\n\n\n\n\nNov 6, 2025\n\n18 min\n\n\n\n\n\n\naRt-o Pollo\n\n\n\nR\n\nart\n\n2025\n\n\n\nI explore a strange light-physics idea and turn it into generative art in R.\n\n\n\n\n\nFeb 21, 2025\n\n8 min\n\n\n\n\n\n\nR love you, R hate you\n\n\n\nR\n\nprogramming\n\n2025\n\n\n\nI‚Äôve grown to loveR despite many of its quirks. Perhaps this is just Stockholm Syndrome.\n\n\n\n\n\nFeb 21, 2025\n\n12 min\n\n\n\n\n\n\nReduce friction for creating Quarto blog posts\n\n\n\nworkflow\n\nshell\n\n2024\n\n\n\nI teach myself how to code simple shell scripts to automate some annoying tasks\n\n\n\n\n\nNov 17, 2024\n\n6 min\n\n\n\n\n\n\nRethinking my approach to computational projects and reproducibility\n\n\n\nworkflow\n\nreproducibility\n\nR\n\ngit\n\ntargets\n\nproject management\n\n2024\n\n\n\nI‚Äôve spent more time debugging my workflow than actually doing research. There has to be a better way.\n\n\n\n\n\nNov 17, 2024\n\n9 min\n\n\n\n\n\n\nTaking a stand on open peer review\n\n\n\nacademic publishing\n\npeer review\n\nopen science\n\n2024\n\n\n\nI will no longer review for journals that do not publish reviews alongside the work\n\n\n\n\n\nAug 4, 2024\n\n2 min\n\n\n\n\n\n\nIntroducing the Bayesian Measurement Modeling R Package (bmm)\n\n\n\nR\n\nmodeling\n\nbayesian\n\nR package\n\n2024\n\n\n\nMaking Bayesian measurement modeling in psychology accessible, reliable & efficient\n\n\n\n\n\nJun 13, 2024\n\n14 min\n\n\n\n\n\n\nLocally Ignoring Git Files Without Affecting Others‚Äô .gitignore\n\n\n\ngit\n\nGitHub\n\nreproducibility\n\nworkflow\n\ncollaboration\n\n2024\n\n\n\nHow to exclude files from version control without affecting other developers‚Äô .gitignore configuration\n\n\n\n\n\nMay 24, 2024\n\n4 min\n\n\n\n\n\n\nWorking with multiple versions of an R package\n\n\n\nR\n\nreproducibility\n\npackage management\n\nR package\n\n2024\n\n\n\nAfter being dissatisfied with existing solutions, I wrote a package to do that\n\n\n\n\n\nMar 3, 2024\n\n7 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/git-local-ignore/index.html",
    "href": "posts/2024/git-local-ignore/index.html",
    "title": "Locally Ignoring Git Files Without Affecting Others‚Äô .gitignore",
    "section": "",
    "text": "I often collaborate on git projects and I find that I want to have a folder or some files stored locally in the repo but that I don‚Äôt want to be tracked by git. Obviously I could add them to .gitignore, but then I have two options:\n\ncommit the .gitignore file and push it to the repo. For public projects to which I‚Äôm contributing small changes, this is not ideal as it clutters the repo with my personal configuration (and it‚Äôs not very polite to the repo owner)\nnot commit the .gitignore file and keep it only locally. This is not ideal either, as I have to remember to not commit it every time I make a change to the repo\n\nA specific example is when I collaborate on R packages. There are several .Rprofile files which R uses to load some settings at startup. I have a bunch of convenience configurations in my user .Rprofile which helps me with my workflow. The problem is that if there is an .Rprofile file in the project root, R will use that one instead of my user .Rprofile. A workaround is to add some lines to the project .Rprofile to source my user .Rprofile, but I don‚Äôt want to commit these lines to the project .Rprofile.\nThere is an easy solution to this, but I always forget the syntax and after the 4th time I had to look it up, I decided to write it down in a blog post."
  },
  {
    "objectID": "posts/2024/git-local-ignore/index.html#the-problem",
    "href": "posts/2024/git-local-ignore/index.html#the-problem",
    "title": "Locally Ignoring Git Files Without Affecting Others‚Äô .gitignore",
    "section": "",
    "text": "I often collaborate on git projects and I find that I want to have a folder or some files stored locally in the repo but that I don‚Äôt want to be tracked by git. Obviously I could add them to .gitignore, but then I have two options:\n\ncommit the .gitignore file and push it to the repo. For public projects to which I‚Äôm contributing small changes, this is not ideal as it clutters the repo with my personal configuration (and it‚Äôs not very polite to the repo owner)\nnot commit the .gitignore file and keep it only locally. This is not ideal either, as I have to remember to not commit it every time I make a change to the repo\n\nA specific example is when I collaborate on R packages. There are several .Rprofile files which R uses to load some settings at startup. I have a bunch of convenience configurations in my user .Rprofile which helps me with my workflow. The problem is that if there is an .Rprofile file in the project root, R will use that one instead of my user .Rprofile. A workaround is to add some lines to the project .Rprofile to source my user .Rprofile, but I don‚Äôt want to commit these lines to the project .Rprofile.\nThere is an easy solution to this, but I always forget the syntax and after the 4th time I had to look it up, I decided to write it down in a blog post."
  },
  {
    "objectID": "posts/2024/git-local-ignore/index.html#the-solution",
    "href": "posts/2024/git-local-ignore/index.html#the-solution",
    "title": "Locally Ignoring Git Files Without Affecting Others‚Äô .gitignore",
    "section": "The Solution",
    "text": "The Solution\nThe solution depends on the state of the file.\n\nIf the file is not yet tracked by git (new file)\nIf this is a new file that is yet untracked by git, you can just add it to the local .git/info/exclude file. This file is not tracked by git and is specific to your local repo. You can add the file to this file and it will be ignored by git. This follows the same syntax as the .gitignore file. You can do this manually by opening the file and adding the file path to it, or you can do it with the following command:\necho \"&lt;file&gt;\" &gt;&gt; .git/info/exclude\nwhere &lt;file&gt; is the path to the file you want to exclude.\n\n\nIf the file is already tracked by git\nIn addition to adding the file to the local .git/info/exclude file, you also need to remove the file from the git index. This can be done with the following command:\ngit update-index --skip-worktree &lt;file&gt;\nif you change your mind and want to track this file, you can do so with the following command:\ngit update-index --no-skip-worktree &lt;file&gt;"
  },
  {
    "objectID": "posts/2024/git-local-ignore/index.html#define-an-alias-for-easy-access",
    "href": "posts/2024/git-local-ignore/index.html#define-an-alias-for-easy-access",
    "title": "Locally Ignoring Git Files Without Affecting Others‚Äô .gitignore",
    "section": "Define an alias for easy access",
    "text": "Define an alias for easy access\nI find that I use this command often enough to warrant an alias. You can run the following commands to add an alias to your git configuration:\ngit config --global alias.ignore 'update-index --skip-worktree'\ngit config --global alias.unignore 'update-index --no-skip-worktree'\ngit config --global alias.ignored 'git ls-files -v | grep \"^S\"'\nand then you can use the following commands to ignore and unignore files:\ngit ignore &lt;file&gt;\ngit unignore &lt;file&gt;"
  },
  {
    "objectID": "posts/2024/git-local-ignore/index.html#putting-it-all-together-an-example",
    "href": "posts/2024/git-local-ignore/index.html#putting-it-all-together-an-example",
    "title": "Locally Ignoring Git Files Without Affecting Others‚Äô .gitignore",
    "section": "Putting it all together (an example)",
    "text": "Putting it all together (an example)\nLet‚Äôs say I want to contribute code to an R package which is developed on GitHub. I can fork the repo and clone it to my local machine. The package has an .Rprofile file which overwrites my user configuration. I have a bunch of convenience configurations in my user .Rprofile which I want to use when working on this project. I can add the following lines to the project .Rprofile to source my user .Rprofile:\ntry(rprofile::load())\nI can then add the project .Rprofile to the local .git/info/exclude file:\necho \".Rprofile\" &gt;&gt; .git/info/exclude\nand finally tell git to ignore the file locally (assuming I already have the alias defined):\ngit ignore .Rprofile"
  },
  {
    "objectID": "posts/2024/r-multiple-package-versions/index.html",
    "href": "posts/2024/r-multiple-package-versions/index.html",
    "title": "Working with multiple versions of an R package",
    "section": "",
    "text": "Have you ever wanted to test whether your code works with multiple versions of an R package? Or compare how the behavior of certain functions has changed? There are several ways to do that, each involving a lot of setup.\n\n\nThe way to work with different package versions in base R requires you to manually specify a folder in which to install each version. Let‚Äôs assume your current version of the package stringr is 1.4.0, and you want to install separately the latest version (as of the time of this writing, 1.5.1). The following will install stringr in a folder stringr-new in your user home folder:\ndir.create('~/stringr-new')\ninstall.packages('stringr', lib = \"~/stringr-new\")\nAfterwards, you can load the default package with\nlibrary(stringr)\nand the new version with\nlibrary(stringr, lib.loc = \"~/stringr-new\")\nA few issues with this approach:\n\nYou have to manually specify the folder for each version. And remember what it was when you want to use it again.\nYou have to remember to specify the lib.loc argument every time you want to use the new version.\nYou have to remember to detach the old version before loading the new one.\nYou cannot install a specific version of a package from CRAN. You have to download the tarball from CRAN, extract it, and install it from the extracted folder.\nYou have to do this one by one for each package you want to test.\n\n\n\n\nWe can augment the base R approach with the remotes package, which provides the install_version function. This function allows you to install a specific version of a package from CRAN. The following will install stringr version 1.5.1 in a folder stringr-new in your user home folder:\ndir.create('~/stringr-new')\nremotes::install_version('stringr', version = '1.5.1', lib = \"~/stringr-new\")\nLoading the packages is the same as before. This approach solves the issue of having to download and install a specific version of a package from CRAN. However, it does not solve the other issues.\n\n\n\nThe renv package is a package manager for R. It allows you to create a project-specific library, and to specify the versions of packages you want to use in a renv.lock file. It allows for a completely reproducible environment, and is the best solution for that purpose. However, it can be an overkill if you just want to test a few versions of a package. For an introduction to renv, see this blog post."
  },
  {
    "objectID": "posts/2024/r-multiple-package-versions/index.html#the-problem",
    "href": "posts/2024/r-multiple-package-versions/index.html#the-problem",
    "title": "Working with multiple versions of an R package",
    "section": "",
    "text": "Have you ever wanted to test whether your code works with multiple versions of an R package? Or compare how the behavior of certain functions has changed? There are several ways to do that, each involving a lot of setup.\n\n\nThe way to work with different package versions in base R requires you to manually specify a folder in which to install each version. Let‚Äôs assume your current version of the package stringr is 1.4.0, and you want to install separately the latest version (as of the time of this writing, 1.5.1). The following will install stringr in a folder stringr-new in your user home folder:\ndir.create('~/stringr-new')\ninstall.packages('stringr', lib = \"~/stringr-new\")\nAfterwards, you can load the default package with\nlibrary(stringr)\nand the new version with\nlibrary(stringr, lib.loc = \"~/stringr-new\")\nA few issues with this approach:\n\nYou have to manually specify the folder for each version. And remember what it was when you want to use it again.\nYou have to remember to specify the lib.loc argument every time you want to use the new version.\nYou have to remember to detach the old version before loading the new one.\nYou cannot install a specific version of a package from CRAN. You have to download the tarball from CRAN, extract it, and install it from the extracted folder.\nYou have to do this one by one for each package you want to test.\n\n\n\n\nWe can augment the base R approach with the remotes package, which provides the install_version function. This function allows you to install a specific version of a package from CRAN. The following will install stringr version 1.5.1 in a folder stringr-new in your user home folder:\ndir.create('~/stringr-new')\nremotes::install_version('stringr', version = '1.5.1', lib = \"~/stringr-new\")\nLoading the packages is the same as before. This approach solves the issue of having to download and install a specific version of a package from CRAN. However, it does not solve the other issues.\n\n\n\nThe renv package is a package manager for R. It allows you to create a project-specific library, and to specify the versions of packages you want to use in a renv.lock file. It allows for a completely reproducible environment, and is the best solution for that purpose. However, it can be an overkill if you just want to test a few versions of a package. For an introduction to renv, see this blog post."
  },
  {
    "objectID": "posts/2024/r-multiple-package-versions/index.html#introducing-vmisc-pkg_vload",
    "href": "posts/2024/r-multiple-package-versions/index.html#introducing-vmisc-pkg_vload",
    "title": "Working with multiple versions of an R package",
    "section": "Introducing Vmisc: pkg_vload()",
    "text": "Introducing Vmisc: pkg_vload()\nI was dissatisfied with the existing solutions, so I wrote a package to do that. The Vmisc package provides the pkg_vload function, which allows you to load a specific version of a package, or to install it if it is not already installed. You can start by installing and loading the package:\ninstall.packages('Vmisc', repos = c('https://popov-lab.r-universe.dev'))\nlibrary(Vmisc)\nThe function pkg_vload combines the functionality of library(), remotes::install_version(), and dir.create(), and it also allows you to list as many packages as you want. The simplest option, for everyday use, is to specify just the package names, as you would with library():\npkg_vload(stringr, dplyr, ggplot2)\nIf you already have the package installed, it will load the default version. If you don‚Äôt, it will install the latest version from CRAN in the default library. This use case is identical to xfun::pkg_load(), but there is some added functionality for handling different versions of a package.\nTo load a specific version, you can specify the version argument:\npkg_vload(stringr('1.5.1'), dplyr, ggplot2)\nThe function expects a call to the package name, followed by the version in parentheses. It will also recognize if the version you specified is already installed. For example, if you already have stringr version 1.5.1 installed the good old way, it will load it from the default library. And you will see the following output:\n#&gt; Loading required package: stringr\n#&gt; Loading required package: dplyr\n#&gt; Attaching package: ‚Äòdplyr‚Äô\n#&gt; Attaching package: ‚Äòggplot2‚Äô\nBut let‚Äôs say we want to install version 1.0.0 of stringr. We can do that with the following:\npkg_vload(stringr('1.0.0'))\nwhich results in\n#&gt; Downloading package from url: https://cran.rstudio.com//src/contrib/Archive/stringr/stringr_1.0.0.tar.gz\n#&gt; * installing *source* package 'stringr' ...\n#&gt; ** package 'stringr' successfully unpacked and MD5 sums checked\n#&gt; ** using staged installation\n#&gt; ** ## output omitted ##\n#&gt; * DONE (stringr)\n#&gt; Loading required package: stringr\npkg_vload has created a folder stringr-1.0.0 in the default library path, installed the package there, and loaded it from there. The following two folders now coexist:\ndirs = list.dirs(.libPaths(), recursive = FALSE)\ndirs[grepl('stringr', dirs)]\n#&gt; [1] \"C:/Users/vepopo/AppData/Local/R/win-library/4.3/stringr\"\n#&gt; [2] \"C:/Users/vepopo/AppData/Local/R/win-library/4.3/stringr-1.0.0\"\npkg_vload(stringr('1.0.0')) not only installed the package but also loaded it. If you restart your session, you can load either versions by simply using pkg_vload(stringr) or pkg_vload(stringr('1.0.0')).\npkg_vload(stringr('1.0.0')) # for version 1.0.0\npkg_vload(stringr) # for the default version\nBenefits of using pkg_vload:\n\nvectorized: you can load multiple packages at once\nif a package exists, it will be loaded, otherwise it will be installed and loaded\nyou can specify the version of the package you want to load/install, without having to specify the library path (although you can)\nyou can install as many versions of a package as you want, and they will coexist in the same library path\nyou can switch which version will be the default with another function from the package, pkg_switch_default"
  },
  {
    "objectID": "posts/2024/r-multiple-package-versions/index.html#switching-the-default-version-of-a-package",
    "href": "posts/2024/r-multiple-package-versions/index.html#switching-the-default-version-of-a-package",
    "title": "Working with multiple versions of an R package",
    "section": "Switching the default version of a package",
    "text": "Switching the default version of a package\nThe Vmisc package also provides a function to switch the default version of a package. For example, if you have versions 1.0.0 and 1.5.1 of stringr installed, and you want to make 1.0.0 the default, you can do that with the following:\npkg_switch_default('stringr', '1.0.0')\n#&gt; The default version of stringr has been switched to 1.0.0. The previous default version has been renamed to stringr-1.5.1\n#&gt; Please restart R to complete the process.\nWhat this will do is rename the folder stringr to stringr-1.5.1 and stringr-1.0.0 to stringr. After you restart your session, pkg_vload(stringr) or even just library(stringr) will load version 1.0.0. You can also switch back to the default version with:\npkg_switch_default('stringr', '1.5.1')\n#&gt; The default version of stringr has been switched to 1.5.1. The previous default version has been renamed to stringr-1.0.0\n#&gt; Please restart R to complete the process."
  },
  {
    "objectID": "posts/2024/r-multiple-package-versions/index.html#conclusion",
    "href": "posts/2024/r-multiple-package-versions/index.html#conclusion",
    "title": "Working with multiple versions of an R package",
    "section": "Conclusion",
    "text": "Conclusion\nAlthough not a replacement for renv for reproducible environments, Vmisc provides a simple way to work with multiple versions of an R package. It is especially useful for testing and comparing different versions of a package. The package is available on my R-universe and on GitHub. It was inspired by the xfun package, and contains other functions that I found useful in my everyday work. I hope you find it useful too."
  },
  {
    "objectID": "posts/2024/r-multiple-package-versions/index.html#bonus-finding-all-global-options-used-by-a-package",
    "href": "posts/2024/r-multiple-package-versions/index.html#bonus-finding-all-global-options-used-by-a-package",
    "title": "Working with multiple versions of an R package",
    "section": "Bonus: finding all global options used by a package",
    "text": "Bonus: finding all global options used by a package\nThe Vmisc package also provides a function to find all global options used by a package. These are options you can set via options(), but they are rarely well documented in the package documentation. The function packageOptions will list all global options used by a package, and their default values. For example, to find all global options used by the brms package, you can use the following:\npackageOptions('brms')\n\n#&gt; Package brms current options:\n#&gt; \n#&gt; brms.save_pars       :  NULL \n#&gt; mc.cores             :  1 \n#&gt; brms.threads         :  NULL \n#&gt; brms.opencl          :  NULL \n#&gt; brms.normalize       :  TRUE \n#&gt; brms.algorithm       :  \"sampling\" \n#&gt; brms.backend         :  \"rstan\" \n#&gt; future               :  FALSE \n#&gt; brms.file_refit      :  \"never\" \n#&gt; wiener_backend       :  \"Rwiener\" \n#&gt; brms.verbose         :  FALSE \n#&gt; shinystan.rstudio    :  FALSE \n#&gt; brms.plot_points     :  FALSE \n#&gt; brms.plot_rug        :  FALSE \n#&gt; brms.short_summary   :  FALSE \n#&gt; .brmsfit_version     :  NULL \nThe function is experimental - it scrapes the source code of the package to find mentions of getOption(‚Äòsomething‚Äô, default = something). It also does not provide documentation. But I found it useful for reminding myself of the options I can set for a package, and their default values."
  },
  {
    "objectID": "posts/2024/reproducibility-is-hard/index.html",
    "href": "posts/2024/reproducibility-is-hard/index.html",
    "title": "Rethinking my approach to computational projects and reproducibility",
    "section": "",
    "text": "I have to finally admit it to myself - my research workflows have been breaking down and it‚Äôs time to get serious about change. I‚Äôve been trying to incorporate better coding practices, modern reporting pipelines, and reproducible workflows into my research projects, but it‚Äôs been a struggle. The complexity of the tools, the sheer number of moving parts, and the interactions between them have been overwhelming. I‚Äôve spent more time debugging my workflow than actually doing research. There has to be a better way."
  },
  {
    "objectID": "posts/2024/reproducibility-is-hard/index.html#the-comfort-of-old-habits",
    "href": "posts/2024/reproducibility-is-hard/index.html#the-comfort-of-old-habits",
    "title": "Rethinking my approach to computational projects and reproducibility",
    "section": "The Comfort of Old Habits",
    "text": "The Comfort of Old Habits\nFor many years I had relatively stable workflows for project organization and version control of my research projects. They were never perfect, and given the state of reproducibility in academic psychology, likely much better than the status quo. Since 2014, I‚Äôve been using systematic project template folders, project-based organization, programmatic data wrangling, version control with Git and GitHub, and I‚Äôve dabbled in literate programming tools such as Jupyter Notebooks and R Markdown. I‚Äôve taugth these tools in graduate courses. I knew my way around a command line, and on the rare occasions I needed it, I could run demanding analyses on the Carnegie Mellon computing cluster. I was able to get things done efficiently while maintaining a somewhat decent level of reproducibility.\n\n\n\nI‚Äôve been using the same project structure for 10+ years\n\n\nMy comfortable routine started changing last year, gradually at first, as I began working on my first R package. What began as a side project quickly evolved into something that preoccupied me for months. This was partly because I had a fantastic collaborator, working with whom was an exhilarating, idea-bootstrapping experience. But collaborating also meant I had to up my game ‚Äî I couldn‚Äôt rely on the shortcuts and hacks I‚Äôd developed over the years. For the first time, I had to learn about proper documentation, testing, and collaborative workflows.\nWhen you‚Äôre working on a package, you‚Äôre not just writing code for yourself‚Äîyou‚Äôre writing code for others to use. This means writing clear and concise documentation, creating tests to ensure your code works as expected, and developing code that is modular and easy to understand. And you have to do it in a way that makes collaboration less painful."
  },
  {
    "objectID": "posts/2024/reproducibility-is-hard/index.html#the-pitfalls-of-self-taught-coding",
    "href": "posts/2024/reproducibility-is-hard/index.html#the-pitfalls-of-self-taught-coding",
    "title": "Rethinking my approach to computational projects and reproducibility",
    "section": "The Pitfalls of Self-Taught Coding",
    "text": "The Pitfalls of Self-Taught Coding\nThe painful truth is that proper computational skills are rarely taught in academic programs (at least in psychology). Many of us are self-taught, each with our own quirky ways of doing things. Tools and processes that are standard fare in software development are often foreign to us. So, we muddle through, doing the best we can with what we have. My use of GitHub was a glorified Dropbox, and my coding practices, if you could call them that, were a mishmash of concepts I picked up from various tutorials and blog posts. It mostly worked, but even now I am barely able to reproduce my own work from a few years ago. Broken package dependencies, uncertain code order, and the utter lack of systematic documentation have made my old projects a nightmare to revisit.\n\nMaybe I‚Äôm exaggerating a bit, but the reality is that as my career has progressed, my projects have grown in number and complexity, and it‚Äôs become more and more frustrating to keep track of everything. My existing workflow sat squarely in the middle of ‚Äúthe reproducibility iceberg‚Äù - better than most, but I was starting to feel cold.\nThankfully, while formal education in this area still lags behind, the online landscape is now rich with resources. Over the last year, I‚Äôve been trying to incorporate better coding practices, Quarto websites reporting for projects, and renv for package management. It‚Äôs been an uphill battle, consuming a lot of time and energy."
  },
  {
    "objectID": "posts/2024/reproducibility-is-hard/index.html#the-tension-between-efficiency-and-reproducibility",
    "href": "posts/2024/reproducibility-is-hard/index.html#the-tension-between-efficiency-and-reproducibility",
    "title": "Rethinking my approach to computational projects and reproducibility",
    "section": "The Tension Between Efficiency and Reproducibility",
    "text": "The Tension Between Efficiency and Reproducibility\nThe hardest part is not learning new tools but unlearning old habits and deconstructing my mental models of code, data, and reporting. One of the reasons I love coding in R is the incredibly quick iteration cycle and feedback loop. The ability to have an idea and, within minutes, simulate, visualize, and analyze it often feels like a superpower.\nThe problem is that this same superpower also makes it so easy to be sloppy. Who has time and patience for carefully curating a reproducible workflow when that puts a delay between your idea and its realization?\nThe thing is, I am no longer a grad student chasing down any odd idea that comes my way. As fun as the wild west of coding can be, it‚Äôs not sustainable for a long-term research program, especially when other people depend on you. And let‚Äôs be honest, I also need to be kinder to my future self."
  },
  {
    "objectID": "posts/2024/reproducibility-is-hard/index.html#what-a-mess",
    "href": "posts/2024/reproducibility-is-hard/index.html#what-a-mess",
    "title": "Rethinking my approach to computational projects and reproducibility",
    "section": "What a Mess",
    "text": "What a Mess\nCase in point: I just resumed work on a simulation project that I last touched in April. Despite all my best intentions, I was shocked to find that I couldn‚Äôt reproduce a set of figures I had sent to my collaborator at the time. Even worse, it took me a full day to even figure out what I was doing back then.\nWhy did this happen? There are many culprits, but part of it is that while I coded all the simulation scripts locally, I ran the simulations on a cluster because they were very computationally demanding. At the time, I had no established workflow for sharing intermediate data objects between my remote and local codebases. My attempts to reconstruct what had happened have driven me to the brink of madness. Trust me when I say that I‚Äôve spent more time trying to figure out what I did than it would have taken me to redo everything from scratch.\n\n\n\nWhat a mess‚Ä¶\n\n\nTo avoid scenarios exactly like this, in the last six months I‚Äôve been experimenting with the targets package. Targets is a pipeline toolkit for R that helps you manage the dependencies between your scripts and data objects. It‚Äôs a bit like make for R, but with a lot of bells and whistles. I even implemented it for a couple of other projects.\nWhen combined with renv for package management, and Quarto for reporting, it comes close to what I imagine as a nearly ideal scenario: A self-contained research website, with all the code, data, and results in one place, and a clear, reproducible workflow that can be run on any machine. A modern reporting pipeline that is both transparent and efficient."
  },
  {
    "objectID": "posts/2024/reproducibility-is-hard/index.html#the-complexity-conundrum",
    "href": "posts/2024/reproducibility-is-hard/index.html#the-complexity-conundrum",
    "title": "Rethinking my approach to computational projects and reproducibility",
    "section": "The Complexity Conundrum",
    "text": "The Complexity Conundrum\nWhen it works. Sigh.\nMaybe I‚Äôm just getting old, but putting together all these pieces has proven to be a lot more challenging than I anticipated. The learning curve is steep. These are all fantastic tools, with good documentation, and in some respects I feel lucky to be working in a time when such tools are becoming widely available, and there are so many people voluntarily developing them. The open source community is truly a marvel. But the sheer number of moving parts, and the complexity of the interactions between them, is overwhelming.\nGit + Github + renv + Quarto + targets + crew + deployment + credential management + testing + documentation + collaboration + teaching + writing + thinking + living. It‚Äôs a lot.\nOk, I got carried away with the list, but even when just considering the computational parts, each of them comes with lengthy manuals, quirks, and a dizzying amount of options and configurations, and they don‚Äôt always play nice with each other. I feel like I‚Äôm constantly fighting fires, and my self-help tutorials are getting longer and longer. These past 6 months, I‚Äôve spent more time debugging my workflow than actually doing research.\nI‚Äôm not giving up, though. This is a path worth walking. I just can‚Äôt help but feel there must be a better way. I‚Äôve been trying to code up a personal library of tools to automate some of that complexity, but other things keep getting in the way. I just can‚Äôt help but compare this state of affairs to the relatively much smoother package development workflows, largely thanks to devtools and usethis packages, as well as the consistent framework around it. Package development is not any less complex in scope, but the community has managed to converge on an integrated consistent workflow that just works.\nPart of this is the nature of the beast - research projects are by definition more diverse and less predictable than package development. Every project is a new adventure, with its own unique challenges and requirements, and formats vary so widely between disciplines or even subfields. Part of it is also that these tools are still relatively new."
  },
  {
    "objectID": "posts/2024/reproducibility-is-hard/index.html#the-signal-to-noise-ratio",
    "href": "posts/2024/reproducibility-is-hard/index.html#the-signal-to-noise-ratio",
    "title": "Rethinking my approach to computational projects and reproducibility",
    "section": "The Signal-to-Noise Ratio",
    "text": "The Signal-to-Noise Ratio\nBut even if all the quirks were ironed out, and the process streamlined, another issue, that I rarely see discussed, is just how much more ‚Äúirrelevant‚Äù artefacts are produced in the codebase. When I look at my best attempts to implement the full workflow, I am a bit paralized by the sheer amount of files and code in my repositories that is not directly related to the substance of the research. There are layers upon layers of abstraction, scaffolding, configuration files, and helper functions that are just necessary for the workflow to function.\n\n\n\nIt‚Äôs a lot‚Ä¶\n\n\nTargets itself, for all its truly wonderful functionality, suffers (or at least I do!) from 1) an incredible level of syntax verbosity and 2) too many ways to do the same thing, both of which make it hard to read and understand the code. Flexibility is a double-edged sword.\nEven in an ideal world in which I finally learn all the ins and outs of these packages, I can‚Äôt help but wonder: does the improved reproducibility and transparency pay off if barely anyone else can understand what‚Äôs going on? If I share the code with expert colleagues, I doubt many of them would be able to make sense of it given the signal-to-noise ratio in the codebase. It might as well be written in a different programming language given the layers of abstraction. This perhaps is a temporary problem, as the tools mature and the community converges on best practices, but it‚Äôs a real one.\nThis is not a critique of the tools themselves, but rather a reflection on the complexity of the research process. The tools are a reflection of the complexity of the problem they are trying to solve. But there has to be a better way.\nPS: This post took an unexpected direction. I was planning to write a short introduction to my frustrations, and a detailed guide to targets as I figured out a how to apply it to a new project. I learn best by writing and teaching, and I was hoping that by writing a tutorial I would solidify my understanding of the package. As often happens in writing though, our thoughts, especially those bottled up frustrations, tend to take a life of their own. I guess I might still write the more technical post I was imagining later"
  },
  {
    "objectID": "posts/2025/arto-pollo/index.html",
    "href": "posts/2025/arto-pollo/index.html",
    "title": "aRt-o Pollo",
    "section": "",
    "text": "Imagine a universe where light behaves differently than ours. Rather than diffusing its intensity with distance, and simply increasing it with multiple sources, light in this universe is different. When two light beams meet, their combined intensity depends on the angle between them. Right angles are the best - when two light beams meet at a point perpendicularly, they have the highest possible intensity at that point. As the angle becomes smaller or bigger than 90 degrees (pi/2 radians), the intensity becomes weaker.\nWhy would you imagine such a thing? I don‚Äôt know why I wonder about such things, but I often do. As an example, I drew the image below while thinking about this - we have two ‚Äúsuns‚Äù, the red dots are 90 degree intersections, the blue points are intersections with a smaller angle.\n\nDrawing can only get me so far, but this is an easy enough simulation to do in R, and the results are pretty, especially when we change some of those initial assumptions. I‚Äôve long been a fan of Danielle Navarro‚Äôs approach to generative art, and it has been years since I dipped my toes in these waters, so here goes nothing. This post will be light on details, but hopefully it makes sense. Let‚Äôs explore how this imaginary physics creates some beautiful patterns.\n\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(patchwork)\nlibrary(ambient)\nlibrary(wesanderson)\nlibrary(httpgd)\nlibrary(ggdark)\nlibrary(viridis)\nlibrary(fields)\nset.seed(12345)\n\ntheme_set(\n  dark_theme_void() +\n    theme(plot.background = element_rect(fill = \"#1F1F1F\", color = \"#1F1F1F\"))\n)\n\nWhat things do we need for the simplest possible simulation?\n\ntwo fixed points A and B for our light sources\na way to calculate the angle formed at any other point X where beams from A and B intersect - a way to convert this angle into an intensity value\ncalculate the intensity for many different points in the plane\nvisualize the result\n\nHere are some basic functions to do this:\n\n#' Calculates the angle formed by AX and BX\n#'\n#' @param X a numeric vector of length 2 or a matrix with two columns in which each row represents the x and y coordinates of a point\n#' @param A A numeric vector of length 2 representing the first source\n#' @param B A numeric vector of length 2 representing the second source\n#' @return The angle in radians between vectors A and B with respect to point(s) X.\nangle &lt;- function(X, A, B) {\n  X &lt;- as.matrix(X)\n  if (length(X) == 2) X &lt;- t(X)\n  A &lt;- rep(A, each = length(X)/2)\n  B &lt;- rep(B, each = length(X)/2)\n\n  v1 &lt;- X - A\n  v2 &lt;- X - B\n  dot_product &lt;- rowSums(v1 * v2)\n  norm &lt;- sqrt(rowSums(v1 * v1)) * sqrt(rowSums(v2 * v2))\n  cos_theta &lt;- dot_product / norm\n  acos(cos_theta)\n}\n\n#' Convert angle to intensity\n#'\n#' @param theta A numeric value representing the angle in radians.\n#' @return A numeric value representing the intensity.\nangle_to_intensity &lt;- function(theta) {\n  1 - abs(theta - pi / 2) / (pi / 2)\n}\n\n\n#' Calculate intensity at a point\n#' \n#' @param x a numeric vector of length 2 or a matrix with two columns in which each row represents the x and y coordinates of a point\n#' @param source1 A numeric vector of length 2 representing the first source\n#' @param source2 A numeric vector of length 2 representing the second source\n#' @param angle_transform_fun A function that takes an angle in radians and returns\n#'  a value to be plotted as the coordinate intensity. Default is a linear function of \n#'  the absolute deviation from a right angle.\n#' @return A numeric value in [0, 1] representing the intensity at point x.\nintensity &lt;- function(x, source1, source2, angle_transform_fun = angle_to_intensity) {\n  theta &lt;- angle(x, source1, source2)\n  angle_transform_fun(theta)\n}\n\nWith these functions in hand we can play around. We need two sources:\n\nA &lt;- c(-1, 0)\nB &lt;- c(1, 0)\n\nJust as an illustration, the maximum intensity should be at point (0, 1) which forms a right angle. The minimum intensity should be at any point on the AB line, e.g.¬†(0, 0) as it forms a 180 degree angle:\n\nintensity(c(0, 1), A, B)\n\n[1] 1\n\nintensity(c(0, 0), A, B)\n\n[1] 0\n\n\nNow we need a grid of points where their beams will intersect:\n\ngrid &lt;- expand.grid(\n  x = seq(-2, 2, 0.01),\n  y = seq(-2, 2, 0.01)\n)\n\nand evaluate their intensity:\n\ngrid$intensity1 &lt;- intensity(grid[c(\"x\", \"y\")], A, B)\n\nmain_plot &lt;- grid |&gt; \n  ggplot(aes(x, y, fill = intensity1, color = intensity1)) +\n  geom_raster()\n  \nmain_plot + scale_fill_gradientn(colors = viridis::inferno(256))\n\n\n\n\n\n\n\n\nThat‚Äôs kinda cool. Mostly what I expected, but it‚Äôs pretty. We can try a few more color schemes and plot settings. To make things a bit easier, I want to simultaneously plot multiple versions with different binning granularities:\n\nplot3s &lt;- function(main, pallette) {\n  p1 &lt;- main + \n    scale_fill_stepsn(colors = pallette, n.breaks = 12) + \n    theme(legend.position = \"none\")\n  p2 &lt;- main + \n    scale_fill_stepsn(colors = pallette, n.breaks = 24) + \n    theme(legend.position = \"none\")\n  p3 &lt;- main + \n    scale_fill_gradientn(colors = pallette) + \n    theme(legend.position = \"none\")\n\n  p1 + p2 + p3\n}\n\nFor example with out original palette we get:\n\nplot3s(main_plot, viridis::inferno(256))\n\n\n\n\n\n\n\n\nWith the binned version on the left, I notice something that wasn‚Äôt as obvious in the smooth one: there is more than one circle! In fact, all points that form the same angle with A and B lie on a circle. This wasn‚Äôt immediately obvious to me, but I eventually remembered the inscribed angle theorem from high school geometry. This theorem is a more general case of the well-known Thales‚Äô theorem, which states that all points on a circle form 90-degree angles with any diameter line of the circle. The resulting figure also represents one half of what is known as the Apollonian circles. This realization led me down a Wikipedia rabbit hole, where I read more about alternative coordinate systems like the bipolar coordinate system. At first glance, it seems esoteric and pointless, but it turns out it can simplify many problems that are otherwise too complicated to compute in standard Cartesian coordinate systems.\nAnyway, let‚Äôs go on with making pretty variations on this theme.\n\nplot3s(main_plot, hcl.colors(12, \"YlOrRd\", rev = TRUE))\nplot3s(main_plot, c(\"#00FFFF\", \"#8A2BE2\", \"#FFD700\"))\nplot3s(main_plot, wes_palette(\"Royal1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlright, how about we add some correlated noise for variety?\n\ngaussian_kernel &lt;- function(sigma_x, sigma_y = sigma_x) {\n  size_x &lt;- ceiling(2 * sigma_x) + 1\n  size_y &lt;- ceiling(2 * sigma_y) + 1\n  \n  kernel &lt;- outer(-size_x:size_x, -size_y:size_y, function(x, y) {\n    exp(-(x^2 / (2 * sigma_x^2) + y^2 / (2 * sigma_y^2)))\n  })\n\n  kernel / sum(kernel)\n}\n\nconvolve2d &lt;- function(mat, kernel) {\n  fft_mat &lt;- fft(mat)\n  fft_kernel &lt;- fft(kernel, dim(mat))\n  Re(fft(fft_mat * fft_kernel, inverse = TRUE) / length(mat))\n}\n\nsmooth_matrix &lt;- function(mat, kernel = gaussian_kernel(1)) {\n  pad_kernel &lt;- matrix(0, nrow = nrow(mat), ncol = ncol(mat))\n  pad_kernel[1:nrow(kernel), 1:ncol(kernel)] &lt;- kernel\n  convolve2d(mat, pad_kernel)\n}\n\ngrid_size &lt;- sqrt(nrow(grid))\nnoise &lt;- matrix(rnorm(grid_size^2), grid_size)\nnoise &lt;- smooth_matrix(noise, kernel = gaussian_kernel(10))\n\nNow won‚Äôt you look at that!\n\ngrid$intensity2 &lt;- grid$intensity1 + as.vector(t(noise))\n\nnoise_plot &lt;- grid |&gt; \n  ggplot(aes(x, y, fill = intensity2, color = intensity2)) +\n  geom_raster() +\n  theme(legend.position = \"none\")\n\nplot3s(noise_plot, rev(viridis::inferno(256)))\nplot3s(noise_plot, hcl.colors(12, \"YlOrRd\", rev = TRUE))\nplot3s(noise_plot, c(\"#00FFFF\", \"#8A2BE2\", \"#FFD700\"))\nplot3s(noise_plot, wes_palette(\"Royal1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlright, one last one. Instead of clamping the intensity to be a linear function of how much the angle deviates from 90 degrees, let‚Äôs introduce some oscillations. We‚Äôll also use a larger range of x and y coordinates.\n\nnonlinear_intensity &lt;- function(theta) {\n  deviation &lt;- abs(theta - pi/2) / (pi/2)\n  sin(2 * pi * deviation)^2\n}\n\nbigger_grid &lt;- expand.grid(\n  x = seq(-4, 4, 0.02),\n  y = seq(-4, 4, 0.02)\n)\n\nbigger_grid_size &lt;- sqrt(nrow(bigger_grid)) \nbigger_noise &lt;- matrix(rnorm(bigger_grid_size^2, sd = 2), bigger_grid_size)\nbigger_noise &lt;- smooth_matrix(bigger_noise, kernel = gaussian_kernel(10))\n\nbigger_grid$intensity3 &lt;- intensity(bigger_grid[c(1,2)], A, B, nonlinear_intensity) + \n  as.vector(t(bigger_noise))\n\nnoise_plot_nl &lt;- bigger_grid |&gt; \n  ggplot(aes(y, x, fill = intensity3, color = intensity3)) +\n  geom_raster() +\n  theme(legend.position = \"none\")\n\nplot3s(noise_plot_nl, viridis::inferno(256))\nplot3s(noise_plot_nl, hcl.colors(12, \"YlOrRd\", rev = TRUE))\nplot3s(noise_plot_nl, c(\"#00FFFF\", \"#8A2BE2\", \"#FFD700\"))\nplot3s(noise_plot_nl, grey.colors(2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLast one, for real this time:\n\n# Generate and smooth directional noise\ngrid_size &lt;- sqrt(nrow(grid))\nnoise_matrix &lt;- matrix(rnorm(grid_size^2), nrow = grid_size)\nsmoothed_noise &lt;- smooth_matrix(noise_matrix, kernel = gaussian_kernel(sigma_x = 10, sigma_y = 2))\n\n# Apply noise only on the bottom part of the image and clip intensity between 0 and 1\nis_lower_half &lt;- grid$y &lt; 0\ngrid$intensity4 &lt;- ifelse(\n  is_lower_half,\n  grid$intensity1^0.75 + as.vector(smoothed_noise),\n  grid$intensity1\n) \n\n# somewhat random aesthetic choices\ninside_upper_circle &lt;- (grid$y^2 + grid$x^2 &lt;= 1) & !is_lower_half\ngrid$intensity4 &lt;- pmax(pmin(grid$intensity4, 1), 0)\ngrid$intensity4[inside_upper_circle] &lt;- grid$intensity4[inside_upper_circle]^0.4\n\nstreak_noise_plot &lt;- grid |&gt; \n  ggplot(aes(x, y, fill = intensity4, color = intensity4)) +\n  geom_raster() +\n  theme(legend.position = \"none\")\n\nplot3s(streak_noise_plot, viridis::inferno(256))\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{popov2025,\n  author = {Popov, Vencislav},\n  title = {aRt-o {Pollo}},\n  date = {2025-02-21},\n  url = {https://venpopov.com/posts/2025/arto-pollo/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPopov, Vencislav. 2025. ‚ÄúaRt-o Pollo.‚Äù February 21, 2025.\nhttps://venpopov.com/posts/2025/arto-pollo/."
  },
  {
    "objectID": "posts/2025/r-love-you-r-hate-you/index.html",
    "href": "posts/2025/r-love-you-r-hate-you/index.html",
    "title": "R love you, R hate you",
    "section": "",
    "text": "I do the majority of my coding in R. I‚Äôve grown to love the language despite many of its quirks. Perhaps this is just Stockholm Syndrome as I joked in my ‚ÄúIntro to programming‚Äù class recently, though I genuinely enjoy the language and its functional programming roots.\nThat said, there are things that are objectively bad about it, with defaults that no sane person would choose. The more time I spend writing code for packages rather than data analysis, the more I‚Äôve grown annoyed at how much extra work you need to do to avoid perplexing bugs and behavior. This is partly a weakness of dynamically typed languages - rather than depending on type-checking built into the language, you need to program defensively and explicitly handle user-input and whether it is what you expect it to be (see excellent discussions here and here). It‚Äôs a problem important enough that it has led to the proliferation of many packages that aim to assist you with it - e.g.¬†checkmate, ensurer, tester, assertive, assertr. But another part is just due to some genuinely weird choices built in many of R‚Äôs base functions."
  },
  {
    "objectID": "posts/2025/r-love-you-r-hate-you/index.html#so-what-we-do-get-a-warning-dont-we",
    "href": "posts/2025/r-love-you-r-hate-you/index.html#so-what-we-do-get-a-warning-dont-we",
    "title": "R love you, R hate you",
    "section": "So what, we do get a warning don‚Äôt we?",
    "text": "So what, we do get a warning don‚Äôt we?\nWe sure do, but warnings can be suppressed, just like I did above. It‚Äôs common to suppress output of functions when running many iterations of a chatty function in an analysis script and problems can easily go unnoticed. That‚Äôs exactly what happened recently in our Bayesian measurement modeling R package when a colleage reported a weird bug. We have one computational model that can apply different forms of Luce‚Äôs choice decision rule - a standard version and a version passed through a softmax normalization. Due to this weird way that switch treats factors, the wrong normalization was applied. This is a recent and not yet officially released model, so we are yet to write all input validations. This could have easily gone unnoticed and led to incorrect model specification that nevertheless lets the model run.\nWarnings are not a reliable way to signal undesired behavior. Especially when the documentation of switch‚Äôs warning itself notes that ‚Äútypically the character level is meant‚Äù. Well, if typically a character level is meant, why is the default EXACTLY THE OPPOSITE?!?"
  },
  {
    "objectID": "posts/2025/r-love-you-r-hate-you/index.html#fine-but-you-tell-users-language-should-be-a-character-right",
    "href": "posts/2025/r-love-you-r-hate-you/index.html#fine-but-you-tell-users-language-should-be-a-character-right",
    "title": "R love you, R hate you",
    "section": "Fine, but you tell users ‚Äúlanguage‚Äù should be a character‚Ä¶ right?",
    "text": "Fine, but you tell users ‚Äúlanguage‚Äù should be a character‚Ä¶ right?\nWe do, but here‚Äôs the kicker - R loves to turn character vectors into factors. So much so that disabling such behavior was one of the main motivation behind the development of tibbles. Until R4.0.0, whenever you used a function like read.csv() to read a file as a data.frame, R by default converted character columns to factors. Thankfully now this default has been reversed, but here‚Äôs the deal - NOT EVERYWHERE!\nOne place which I never knew R created factors out of character vectors is expand.grid. Expand.grid is a commonly used function to get a data.frame with all combinations of several variables, which is useful for running models with orthogonality manipulated conditions. E.g.:\n\nconditions &lt;- expand.grid(\n  value = c(1, 100),\n  version = c(\"cs\",\"ss\"), \n  choice_rule = c(\"simple\", \"softmax\")\n)\nconditions\n#&gt;   value version choice_rule\n#&gt; 1     1      cs      simple\n#&gt; 2   100      cs      simple\n#&gt; 3     1      ss      simple\n#&gt; 4   100      ss      simple\n#&gt; 5     1      cs     softmax\n#&gt; 6   100      cs     softmax\n#&gt; 7     1      ss     softmax\n#&gt; 8   100      ss     softmax\n\nCan you tell that version and choice_rule are factors? I‚Äôve used expand.grid for years without knowing that default behavior of expand.grid, and it‚Äôs partly that standard data.frame print method does not differentiate character and factor columns in any way. You can see that indeed we have factors underneath by using str for more details:\n\nstr(conditions, give.attr = FALSE)\n#&gt; 'data.frame':    8 obs. of  3 variables:\n#&gt;  $ value      : num  1 100 1 100 1 100 1 100\n#&gt;  $ version    : Factor w/ 2 levels \"cs\",\"ss\": 1 1 2 2 1 1 2 2\n#&gt;  $ choice_rule: Factor w/ 2 levels \"simple\",\"softmax\": 1 1 1 1 2 2 2 2\n\nYou can examine the documentation, or directly check with formals that expand.grid, just like read.csv has a argument stringsAsFactors, which defaults to TRUE:\n\nformals(expand.grid)\n#&gt; $...\n#&gt; \n#&gt; \n#&gt; $KEEP.OUT.ATTRS\n#&gt; [1] TRUE\n#&gt; \n#&gt; $stringsAsFactors\n#&gt; [1] TRUE\n\nWait, but didn‚Äôt I just write that R4.0.0 solves the problem? As of R4.4.2, it only does that for read.table and data.frame, but not expand.grid\n\nversion$version.string\n#&gt; [1] \"R version 4.4.2 (2024-10-31)\"\nformals(read.table)[\"stringsAsFactors\"]\n#&gt; $stringsAsFactors\n#&gt; [1] FALSE\nformals(data.frame)[\"stringsAsFactors\"]\n#&gt; $stringsAsFactors\n#&gt; [1] FALSE\nformals(expand.grid)[\"stringsAsFactors\"]\n#&gt; $stringsAsFactors\n#&gt; [1] TRUE\n\nYou can of course change that by being explicit about not wanting factors, or use tidyr::expand_grid alternative instead:\n\nexpand.grid(\n  value = c(1, 100),\n  version = c(\"cs\",\"ss\"), \n  choice_rule = c(\"simple\", \"softmax\"),\n  stringsAsFactors = FALSE\n) |&gt; str(give.attr = FALSE)\n#&gt; 'data.frame':    8 obs. of  3 variables:\n#&gt;  $ value      : num  1 100 1 100 1 100 1 100\n#&gt;  $ version    : chr  \"cs\" \"cs\" \"ss\" \"ss\" ...\n#&gt;  $ choice_rule: chr  \"simple\" \"simple\" \"simple\" \"simple\" ...\n\ntidyr::expand_grid(\n  value = c(1, 100),\n  version = c(\"cs\",\"ss\"), \n  choice_rule = c(\"simple\", \"softmax\")\n) |&gt; str()\n#&gt; tibble [8 √ó 3] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ value      : num [1:8] 1 1 1 1 100 100 100 100\n#&gt;  $ version    : chr [1:8] \"cs\" \"cs\" \"ss\" \"ss\" ...\n#&gt;  $ choice_rule: chr [1:8] \"simple\" \"softmax\" \"simple\" \"softmax\" ..."
  },
  {
    "objectID": "posts/2025/r-love-you-r-hate-you/index.html#consistency-is-important",
    "href": "posts/2025/r-love-you-r-hate-you/index.html#consistency-is-important",
    "title": "R love you, R hate you",
    "section": "Consistency is important",
    "text": "Consistency is important\nConsistency is a loaded term. Whether two things are consistent or not necessarily depends on context and goals. One could go on and on about consistent naming styles, consistent argument order, consistent default behaviors, etc. At its core the problem with inconsistent design choices in a programming language is that it makes it very difficult, if not impossible, to build an accurate mental model of how the language works. It‚Äôs more than a bit ironic when programming languages, which are supposed to be the precise counterpart of natural languages, become a similar tangled mess of exceptions you need to learn by heart, just like irregular verbs in English.\nThe main example of this post concerns the concept of control flow in programming. Both if/else statements and case/switch statements aim to accomplish the same goal - to execute different parts of a program depending on a prespecified condition. One would expect that logical comparisons would work the same between those two constructs, but as this example has illustrated, they do not in R. Sometimes complex examples obscure the core problems, so let‚Äôs present it at it‚Äôs simplest form:\n\nx &lt;- factor(\"Peace\", levels = c(\"Violence\", \"Peace\"))\n\nx == \"Peace\"\n#&gt; [1] TRUE\nx == 2\n#&gt; [1] FALSE\nx == \"2\"\n#&gt; [1] FALSE\n\nswitch(x, Peace = \"I choose peace\", Violence = \"I choose violence\")\n#&gt; Warning in switch(x, Peace = \"I choose peace\", Violence = \"I choose violence\"): EXPR is a \"factor\", treated as integer.\n#&gt;  Consider using 'switch(as.character( * ), ...)' instead.\n#&gt; [1] \"I choose violence\"\n\n\n\n\n\nWas Cercei mislead by R? We‚Äôll never know.\n\n\n\nLogical comparisons treat factors as character vectors, switch comparisons treat them as integer values. This is not ok. Sure, you can learn that this is the case, but you shouldn‚Äôt have to.\nThese inconsistencies in base R have led to various attempts at reform, most notably through the tidyverse ecosystem. While the tidyverse strongly emphasizes consistency, it presents its own challenges. Breaking changes are frequent, forcing regular code updates and relearning of functionality. While useful for analysis code, the packages‚Äô interdependence and heavy footprint make them difficult to justify in lightweight, stable package development. Many tidyverse functions on the surface appear to simply wrap basic R operations, unnecessarily expanding the language‚Äôs complexity. The tidyverse‚Äôs impact on the R community remains contentious, sparking numerous debates about its influence (see discussions here, here, here, here, here). My own perspective has evolved from enthusiasm to skepticism, though I still use tidyverse tools when appropriate. I don‚Äôt want to throw away the baby along with the bathwater though - despite its drawbacks, the tidyverse gets one thing right: function APIs should be consistent and avoid ‚Äúmagic‚Äù behavior. R along with its predecessor S, is an old language, and a lot can be forgiven due to its legacy.\nThis situation mirrors broader patterns in programming language evolution. Consider C++: despite being a modern, widely-used language, it carries significant legacy baggage due to its strict backwards compatibility requirements. Newer languages like Rust, free from such constraints, can implement better defaults and more consistent behavior from the start. The tidyverse represents a curious case of attempting to fix a language‚Äôs shortcomings ‚Äòfrom within‚Äô rather than through dialect separation or replacement. While this approach maintains ecosystem cohesion, it creates a complex divide within the R community.\nPython offers an instructive contrast in language evolution. Through semantic versioning, Python made major breaking changes between versions 2 and 3, prioritizing language improvement over backwards compatibility. While the transition wasn‚Äôt painless‚ÄîPython 2 still claimed 25% usage a decade after Python 3‚Äôs 2008 release‚Äîby 2023, Python 2 usage had dropped to just 6% (and the transition plans were announced well in advance - PEP 3000 was published in 2006). R, conversely, maintains strong API stability between major versions. Many of the tidyverse‚Äôs consistency-oriented features could serve as excellent candidates for a base R rewrite if the community embraced the possibility of meaningful breaking changes in major versions. Yes, such transitions can be challenging‚ÄîPython‚Äôs experience demonstrates this‚Äîbut they also show that systematic language improvement is achievable with proper planning and community support.‚Äù"
  },
  {
    "objectID": "posts/2025/r-love-you-r-hate-you/index.html#what-now",
    "href": "posts/2025/r-love-you-r-hate-you/index.html#what-now",
    "title": "R love you, R hate you",
    "section": "What now?",
    "text": "What now?\nFirst, of course, is for me to sit down and do the annoying grunt work of going through all user-facing functions and ensuring that we test and validate every input. There‚Äôs a ton of ways to do it, either with vanilla R and custom functions (stopifnot and match.arg are your friends), or with the help of some validation packages I listed earlier. Then add more unit tests about edge cases to make sure things like this don‚Äôt happen. And so on‚Ä¶ We have done this for a lot of our existing code, but as this example taught me, it‚Äôs easy to forget - and sometimes easy to not know.\nLong-term, however, I‚Äôm becoming more and more interested in programming languages that use a strong static type system. I‚Äôve long taught that static typing is simply annoying - as a self-taught programmer, I haven‚Äôt had the benefit of learning things ‚Äúthe right way‚Äù. Over the last few months I‚Äôve started digging deeper into programming as a core skill, exploring various languages and resources. I‚Äôm growing more and more attuned to the virtues of good type systems, for many other reasons. I‚Äôm sure another post on this topic is incoming at some time. In the meantime:\nif (language == \"R\") {\n  stopifnot(inputs_are_carefully_validated())\n}"
  },
  {
    "objectID": "posts/2026/simple-rules-hard-problems/index.html",
    "href": "posts/2026/simple-rules-hard-problems/index.html",
    "title": "Simple rules, hard problems and the emergence of meaning in mathematics",
    "section": "",
    "text": "I watched another video about the Collatz conjecture last week. Thanks, algorithm. I don‚Äôt even know why I do this to myself. It reliably produces the same mixture of fascination and irritation: the rules are tiny, the behavior is bizarre, and my mind insists that the whole thing should be graspable in some clean way, and then fails to grasp it.\nThis is not a crank post. It‚Äôs barely even about Collatz, which was merely a trigger. And yet, as in many number theory questions, whatever the answer is, there surely is one. The system is fully specified. Either every starting value eventually reaches 1, or some don‚Äôt. Either there is a nontrivial cycle, or there isn‚Äôt. Whether we can find a proof is a separate question.\nOf course, Kurt G√∂del is always waiting like an ominous gargoyle to tell us that some things are undecidable (relative to some standard axiom systems). But even if that were the case for Collatz, there is still a fact of the matter: within whatever framework you pick, the statement is either provable, disprovable, or independent. In any case, it‚Äôs not like the conjecture floats in some metaphysical fog. It is determined by properties of the positive integers, together with the transition rule.\nWhat interests me, though, is less the conjecture itself than what it does to my head. It‚Äôs a very peculiar experience: the sense that something is so ‚Äúobviously constrained‚Äù and yet infuriatingly inacessible and hard. I don‚Äôt mean ‚Äúhard‚Äù as in ‚Äúrequires 200 pages of technical machinery.‚Äù I mean hard as in: I repeatedly form ideas that feel coherent and so obviously about to crack the problem once and for all; and then they of course evaporate or lead to the same dead ends.\nAnd that experience, is what pushed me into a broader line of thought: what are mathematical objects, what does it mean for them to have properties, and what does it feel like (cognitively) to search for structure in a system when you don‚Äôt yet have the right concepts to see it? What does it mean to understand something that by some accounts doesn‚Äôt really exist?\nWhatever the ontological status of mathematical objects ultimately is, Platonic, fictionalist, structuralist, model-theoretic, ‚Äújust‚Äù patterns in computation, we can say a few things with relative certainty. One of them is that mathematical objects and structures are essentially relational. They do not have meaningful properties in isolation. Their ‚Äúproperties‚Äù are about the role they play inside a system: how they behave under defined operations, and what they entail when they interact with other objects."
  },
  {
    "objectID": "posts/2026/simple-rules-hard-problems/index.html#meaning-without-pointing",
    "href": "posts/2026/simple-rules-hard-problems/index.html#meaning-without-pointing",
    "title": "Simple rules, hard problems and the emergence of meaning in mathematics",
    "section": "Meaning without pointing",
    "text": "Meaning without pointing\n\nA mathematical object is what it does inside a system of relations and operations\n\nHuman language lets us form grammatically clean statements that carry no content. I can say ‚ÄúX is roupy‚Äù just as easily as I can say ‚Äú2 is even.‚Äù But unless ‚Äúroupy‚Äù connects to something, I haven‚Äôt said anything.\nIf I define ‚Äúroupy‚Äù as ‚Äúpaternally upy,‚Äù I haven‚Äôt fixed the problem. I‚Äôve just stepped one move deeper into a dictionary loop. The words fit together, but nothing constrains anything.\nWith everyday words, we eventually escape this loop by pointing to the world. Some terms get grounded in perception and action. Even if definitions remain fuzzy, there‚Äôs friction: you can be wrong about what ‚Äúred‚Äù applies to, and the world pushes back.\nMathematics doesn‚Äôt get this kind of grounding, at least not in the same direct way. The mathematician isn‚Äôt allowed to point at ‚Äútwo-ness‚Äù floating in the air. The only thing left is this: a property of X becomes meaningful only insofar as it connects X to other objects through formal operations and relations. In other words: if the definition doesn‚Äôt place X in a structure, it‚Äôs not doing anything.\nThe symbols 1, 2, 3, ‚Ä¶ only mean something, i.e., have properties, once we define operations and rules: Peano‚Äôs axioms, or Church numerals, or a set-theoretic construction. Only within such a system does the statement ‚Äú6 is divisible by 2‚Äù have content, because ‚Äúdivisible by 2‚Äù can be unpacked and checked under the representation you‚Äôre using. It defines a relation between objects of the same class (6 and 2) and the outcome of an operation involving them (like ‚Äúremainder 0‚Äù).\nThe point is not that ‚Äúdefinitions matter.‚Äù That‚Äôs trivial. The point is that mathematical meaning is operational and relational. It lives in what the symbol lets you do and what it forces to be true."
  },
  {
    "objectID": "posts/2026/simple-rules-hard-problems/index.html#a-seemingly-silly-object-called-a-bazz",
    "href": "posts/2026/simple-rules-hard-problems/index.html#a-seemingly-silly-object-called-a-bazz",
    "title": "Simple rules, hard problems and the emergence of meaning in mathematics",
    "section": "A seemingly silly object called a bazz",
    "text": "A seemingly silly object called a bazz\nTo make this feel less like a slogan, let‚Äôs take a deliberately silly example.\nImagine an object called a bazz. It interacts with pins. There are three operations:\n\nkuua takes a pin and a bazz and returns a bazz\nouz takes a bazz and returns a bazz plus a pin\nheff takes a bazz and returns a pin\n\nAlso:\n\nyou can always fish a bazz from the void\nyou can ask a bazz whether it is a jazz, and it must answer by nodding or shaking\n\nAnd bazzes satisfy these rules:\n\nIf you kuua a bazz with a pin and then immediately ouz it, you get the original bazz and pin back.\nIf you heff a bazz after kuua-ing it with a pin, you get back that pin ‚Äì and if you then ouz the bazz, you mysteriously get another free copy of the same pin and the original bazz.\nA freshly fished void bazz always nods when asked if it is a jazz. After kuaa-ing it with a pin it shakes, and once it shakes, it keeps shaking.\n\nIt sounds like nonsense (it is), but it is also just a stack, an Abstract Data Type, in disguise: push(), pop(), peek(), create(), isEmpty():\n\n\n\nBazz operation\nStack operation\nEffect\n\n\n\n\nkuua(pin, bazz)\npush(item, stack)\nAdd a pin to the bazz\n\n\nouz(bazz)\npop(stack)\nRemove and return the top pin\n\n\nheff(bazz)\npeek(stack)\nView the top pin without removing it\n\n\nfish()\ncreate()\nMake a new empty bazz/stack\n\n\nis jazz?\nisEmpty()\nCheck if the bazz/stack has no pins\n\n\n\nWhile the third column of this table clearly articulates the intended effect of the operation, it is nothing more than a ‚Äúdesign specification‚Äù: what we want it to do and mean. It is the equational rules that endow the system with these behaviors:\n\nouz(kuua(p, b)) = (b, p) ‚Äî If you push a pin onto a bazz and immediately pop, you get back the original bazz and the pin\nheff(kuua(p, b)) = p ‚Äî If you peek at a bazz after pushing a pin, you see that pin\njazz(fish()) = true ‚Äî A freshly created bazz is empty\njazz(kuua(p, b)) = false ‚Äî Any bazz with at least one pin is not empty\n\nThat‚Äôs it - the system is fully specified, regardless of the symbols we use, and the helpful descriptions we attach them and to the rules that govern them. Notice that nothing above ever specifies anything about the internal mechanism of any operation; what is more, it doesn‚Äôt even specify the outcome of individual operations, except for what type of object it returns. It only specifies how the behavior of some rules affects others.\nWhat I like about this example is how it separates three things that we often blur together:\n\nThe interface: the operations you‚Äôre allowed to do.\nThe axioms: the laws those operations must satisfy.\nThe interpretation: the story we tell ourselves (‚Äúit stores things,‚Äù ‚Äúit has an inside,‚Äù ‚Äúit grows,‚Äù etc.).\n\nThe axioms never say ‚Äúlast-in-first-out.‚Äù, ‚Äúinside.‚Äù, nor ‚Äústorage.‚Äù And yet if you take the rules seriously, you can prove an emergent structural truth, a consequence that wasn‚Äôt spelled out:\n\nIf you kuua ten pins in a row and then repeatedly ouz, you recover the pins in reverse order.\n\nAt the same time, some questions you‚Äôre tempted to ask are not merely unanswered; they‚Äôre not even well-formed in the language of the system. For example: ‚Äúwhat happens to the pins inside the bazz?‚Äù There is no ‚Äúinside‚Äù predicate. That question sneaks in an external metaphor (containers, interiors) that the abstract description never granted you.\nThis is a genuine constraint on cognition: we naturally reach for metaphors that feel meaningful, and those metaphors can be helpful, but they can also generate pseudo-questions that the formal structure does not support. And I think this distinction between what the system actually says, what it implies, and what our metaphors tempt us to ask is one of the most useful lenses I‚Äôve found for thinking about mathematics."
  },
  {
    "objectID": "posts/2026/simple-rules-hard-problems/index.html#the-integers-as-a-deceptively-thin-interface",
    "href": "posts/2026/simple-rules-hard-problems/index.html#the-integers-as-a-deceptively-thin-interface",
    "title": "Simple rules, hard problems and the emergence of meaning in mathematics",
    "section": "The integers as a deceptively ‚Äúthin‚Äù interface",
    "text": "The integers as a deceptively ‚Äúthin‚Äù interface\nNow we can return to the question that started all of this.\nIf mathematical objects are defined by their role in a web of relations, and if that web contains emergent structure that may be deeply non-obvious, then we can ask:\nIs there some deep structural relation among the integers that we have simply not yet uncovered? A structure that plays little to no role in most of our everyday experience with numbers, a structure we cannot currently ‚Äúsee,‚Äù but that must be there ‚Äì because the truth of these edge-case statements depends on it.\nThink about how many of the hardest problems in mathematics are absurdly simple statements about the most elementary object imaginable: the counting numbers.\nOne apple, two apples, three apples. Each of my three kids can have one. If the dog steals one, I either have to cut them or only give apples to two kids. Or my husband and I can enjoy them ourselves. This is the mental world in which the natural numbers first appear: a thin, practical sequence. Knots on a rope.\nThen you learn that primes are numbers divisible only by themselves and 1. Simple definition. A few steps from axioms to an algorithm. Not useful for most of human history; then suddenly the foundation of modern cryptography. And Euclid proves, 2300 years ago, that you‚Äôll never find the biggest prime. There will always be another. Infinitely many.\nBut then you hit questions that look just as simple and fall off a cliff:\n\nAre there infinitely many twin primes? Unknown.\nIs every even number the sum of two primes? Unknown.\nDoes the rule ‚Äúif (x) is even, divide by 2; otherwise multiply by 3 and add 1‚Äù always reach 1? Unknown.\n\nBefore you take these questions seriously, it feels like there is almost no structure to whole numbers at all. They feel like the least interesting object: a mere successor relation repeated forever. And yet the true structure runs deep, as deep as mathematical structures go, and contains relations that feel completely disconnected from the ‚Äúone apple, two apples‚Äù story.\nQuadratic reciprocity. p-adics. Modular forms. The list goes on.\nAnd this is the part that always hits me with the same weird emotion: these consequences were ‚Äúthere‚Äù long before we had names for them, as certainly in 100,000 BC when the first humans were learning to count as it is today in number theory textbooks. We just didn‚Äôt know it. It still feels like an utter miracle that such complexity was hiding there as an inevitable consequence of logic and formal rules.\nWe know for sure that even extremely sophisticated facts about integers (like Fermat‚Äôs Last Theorem) ultimately derive from whatever basic axioms you accept. They have to. There is no other source of truth available. Yet finding that path took centuries and some of the best minds in history.\nIt seems that so much of the complexity arises because the interaction between addition (linear structure) and multiplication (multiplicative/prime structure) is surprisingly ‚Äúthick.‚Äù Most of the deep mysteries in number theory, live exactly in the friction between these two operations. And still, unless you really think about it, on the surface there appears to be nothing particularly interesting about this interaction. Multiplication distributes over addition. They feel separable, almost independent, and yet that feeling is completely misleading."
  },
  {
    "objectID": "posts/2026/simple-rules-hard-problems/index.html#what-collatz-does-to-my-mind",
    "href": "posts/2026/simple-rules-hard-problems/index.html#what-collatz-does-to-my-mind",
    "title": "Simple rules, hard problems and the emergence of meaning in mathematics",
    "section": "What Collatz does to my mind",
    "text": "What Collatz does to my mind\nThis is where Collatz comes back in, not as the main topic, but as a trigger.\nThe thing I find psychologically striking is that the Collatz rule has a certain interface-thinness to it. It‚Äôs made of the simplest arithmetic operations imaginable (parity, division by 2, multiplication, addition). And yet whatever global behavior it has is not something I can see directly from those operations in the way I can see, say, why a stack is last-in-first-out once I have the right lens.\nWhen I tried to think seriously about it last year, the recurring experience was this:\n\nI‚Äôd have an idea that felt like it captured the behavior\nit would feel obviously promising for a few minutes\nand then, as soon as I tried to formalize it, the idea would dissolve into fog\n\nThe dissolving is the interesting part. It‚Äôs like my mind keeps proposing consequences or candidate invariants that are almost_meaningful. And then I discover they‚Äôre not stable under the operations, or not expressible in the right language, or they fail on some annoying corner case that I can‚Äôt rule out. Or most commonly, just lead me to a gaping hole between the properties of numbers I think are relevant to the problem, and how the numbers acted in this system.\nThis feels, to me, like a lesson about cognition as much as about mathematics. And it also makes me appreciate what fundamental mathematical progress often is: not ‚Äúmore cleverness,‚Äù but deep and novel structural insight - discovering the right language, the right predicates, invariants, decompositions, and abstractions in which the problem becomes legible."
  },
  {
    "objectID": "posts/2026/simple-rules-hard-problems/index.html#final-notes",
    "href": "posts/2026/simple-rules-hard-problems/index.html#final-notes",
    "title": "Simple rules, hard problems and the emergence of meaning in mathematics",
    "section": "Final notes",
    "text": "Final notes\nI wasted a month of my life on Collatz last year, despite reading all the jokes and warnings. Never again, at least not in that mode.\nBut I don‚Äôt regret the detour entirely, because it pushed me to think about the gap between a formal specification and the space of meanings we think we‚Äôre entitled to attach to it.\nThe bazz story is a toy version of this. A few equational laws define an interface. From that interface, real structure follows by logical consequence. At the same time, many of the questions that feel most natural (‚Äúwhat‚Äôs inside?‚Äù ‚Äúwhere do the pins go?‚Äù) are not deep mysteries, but category errors.\nSomething similar happens, I think, whenever we look at an austere set of rules like arithmetic, a dynamical map, a formal grammar, rewriting rules, celluar automata, and then try to reason about it with mental imagery that isn‚Äôt actually tied to the operations. We drift toward stories, we drift toward pictures, we drift toward ‚Äúit must behave like‚Ä¶‚Äù And sometimes those stories guide discovery. But sometimes they generate pseudo-problems and pseudo-solutions: things that feel meaningful only because we imported them.\nSo for me the interesting thing about Collatz is not the conjecture itself. It‚Äôs what it highlights about mathematical meaning. If meaning is relational, constituted by inferential role inside a structure, then ‚Äúunderstanding‚Äù is about having the right conceptual vocabulary. The vocabulary in which statements become connected to consequences you can control.\nAnd that vocabulary is not guaranteed to sit near the surface of the original definition. Sometimes it‚Äôs ridiculously far away. The integers are the canonical example: successor and induction look like a thin interface, and yet the emergent structure is vast, deep and even today poorly understood. The fact that we needed centuries to discover much of that structure for one of the most fundamental everyday concepts is evidence that the inferential consequences of even a small axiomatic core can be profoundly non-obvious."
  }
]